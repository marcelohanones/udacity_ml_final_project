{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Super_Analytica(object):\n",
    "    WHO_LIST = []    #. classe Super nao fica registrada na WHOLIST.\n",
    "    evaluations_all = {}\n",
    "    \n",
    "    def __init__(self, my_name):\n",
    "        self.my_name = my_name\n",
    "#         self.WHO_LIST = []\n",
    "        \n",
    "    def insert_member(self):   # only used in the Sub init\n",
    "        Super_Analytica.WHO_LIST.append(self.my_name)  ## APAGAR DEPOIS\n",
    "#         if not self.my_name in Super_Analytica.WHO_LIST:\n",
    "#             Super_Analytica.WHO_LIST.append(self.my_name)\n",
    "#             print('\\n')\n",
    "#             print(self.my_name, '  Instance Created')\n",
    "#         else:\n",
    "#             raise ValueError(self.my_name, ' This instance is already in WHOLIST. Or pick another name.')\n",
    "            \n",
    "    def plot_ranking(self, chosen=None, var = 'score'):\n",
    "        '''chosen must be a list. var must be a single value string'''\n",
    "#         var = 'score'\n",
    "        data = pd.DataFrame(Super_Analytica.evaluations_all).T\n",
    "#         return data\n",
    "        if chosen:\n",
    "            data = data.loc[chosen, :]\n",
    "        data.sort_values('score', inplace=True)\n",
    "        data.reset_index(inplace=True) # in order to display the values in text at the top of each barplot, the index need to be numeric.\n",
    "        data.rename(columns = {'index': 'my_name'}, inplace=True)\n",
    "        plt.clf()\n",
    "        g = sns.barplot(data[var], data.my_name, orient='h', palette=\"spring\" )\n",
    "        for index, row in data[var].iteritems():\n",
    "            g.text(row, index, round(row,2), color='black', ha=\"right\") \n",
    "        plt.title(var.upper())\n",
    "        \n",
    "    def make_models_list(self, chosen):\n",
    "        '''return a list of tuples for Voting. Chosen must be a list'''\n",
    "        data = pd.DataFrame(Super_Analytica.evaluations_all).T\n",
    "        s = data.loc[chosen, ['best_estimator']] # select the column of interest\n",
    "        s = s['best_estimator'].map(lambda x: x.steps[-1][1]) # within best_estimator object, retrieve the model instance\n",
    "        return list(s.iteritems())\n",
    "    \n",
    "    \n",
    "    def extract_model_instance(self, my_name):\n",
    "        '''my_name must be a single value string. \n",
    "        Returns the model instance out of the pipeline. ( pipelilne stored in evaluaions all) \n",
    "        Intended to export the fitted model and it`s best perams for reuse (bagging).\n",
    "        '''\n",
    "        my_name_no_under = my_name.split('_')[0]\n",
    "        return Super_Analytica.evaluations_all[my_name]['best_estimator'].named_steps[my_name_no_under]\n",
    "        \n",
    "\n",
    "    def pkl_W_evaluations_all(self, filename):    # save evaluation all to disk\n",
    "        '''filename is a string like KNN_1'''\n",
    "        pickle_w = open('../pkl/'+ filename + '.pkl', 'wb')\n",
    "        obj = Super_Analytica.evaluations_all\n",
    "        pickle.dump(obj, pickle_w, pickle.HIGHEST_PROTOCOL)\n",
    "        pickle_w.close()\n",
    "        print('Done. Pickled to disk')\n",
    "        \n",
    "    def pkl_R(self, filename):     # import a pickle into the 'cloud'\n",
    "        ''' filename must be a string like KNN_1\n",
    "        import a pickle into the 'cloud'\n",
    "        '''\n",
    "#         pickle_r = open('../pkl/'+ filename + '.pkl', 'rb')\n",
    "#         back_from_dead = pickle.load(pickle_r)\n",
    "        back_from_dead = joblib.load('../pkl/clf_'+ filename + '.pkl') \n",
    "        #check duplicates and insert into evaluations_all\n",
    "        keys_to_import = list(back_from_dead.keys())\n",
    "        keys_members = list(Super_Analytica.evaluations_all.keys())\n",
    "        Super_Analytica.evaluations_all.update(back_from_dead)\n",
    "        print('Done. ' + filename + ' evaluation pickled from pisk')\n",
    "        \n",
    "        if any(i in keys_members for i in keys_to_import):\n",
    "            print (' One or more evaluations were overwriten')\n",
    "   \n",
    "        \n",
    "    def get_members_evaluations_all(self):\n",
    "        return Super_Analytica.evaluations_all.keys() # pega somente os modelos que acionaram evaluate_HP\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            ##############     PLOTTING SECTION     ###############\n",
    "    \n",
    "    def plot_learning_curve(self, load_saved=True, chosen=None):  #loop over\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list of members of evaluations_all\n",
    "        if chosen == None, it will process all the models in evaluations all\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members)) # all chosen in evaluations_all\n",
    "            list_members = chosen # check if chosen are in evaluations all\n",
    "            list_members.sort()     \n",
    "        \n",
    "        for name in list_members :\n",
    "            path = '../pkl/l_curve_'+ name + '.png'\n",
    "            if load_saved == True:\n",
    "#                 print(type(name))\n",
    "                print('opening the image',  name)\n",
    "        \n",
    "                display(Image(path))\n",
    "            else:\n",
    "                pipe = Super_Analytica.evaluations_all[name]['pipe_best']\n",
    "                self.helper_plot_learning_curve(pipe, name, path)\n",
    "            \n",
    "\n",
    "    def helper_plot_learning_curve(self, pipe, name, path):    \n",
    "        sizes = np.linspace(0.2, 1.0, 5)\n",
    "        viz = LearningCurve(pipe, train_sizes=sizes, scoring='roc_auc')\n",
    "        viz.fit(X_train, y_train)\n",
    "        viz.poof(outpath=path)\n",
    "        print('saving l_curve_ image for ',  name)\n",
    "        plt.clf()\n",
    "#     SE o X_train é texto, como pode essa porra rodar ? <------\n",
    "# nao deveria vetorizar antes ?\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    def plot_validation_curve(self, chosen=None):\n",
    "        '''\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "        \n",
    "        for name in list_members:\n",
    "            self.helper_plot_validation_curve(name)\n",
    "        \n",
    "    \n",
    "    def helper_plot_validation_curve(self, my_name):\n",
    "        \n",
    "        data4 = Super_Analytica.translate(self, my_name)\n",
    "#         return data4\n",
    "        # ATENTION: hp.choice with 5 items or less will be taken as categorical and removed. \n",
    "#         col_rid = [name for name, vals in data4.iteritems() if set(vals.unique()).issubset(set([0,1,2]))]    # avoid columns of no numerical hp.choice. ones that are made of strings as alternatives.\n",
    "#         col_rid = [] \n",
    "#         col_rid.append('losses')\n",
    "#         print(col_rid)\n",
    "#         print(data4.columns)\n",
    "        # iterar sobre data4 e data5 para plotar\n",
    "        # clf_2 = data.best_estimator.get_values()[0]\n",
    "        clf_2 = Super_Analytica.evaluations_all[my_name]['best_estimator']\n",
    "        for param2, vals2 in data4.iteritems(): #iterar sobre o data4 que é o geral que contem todas as colunas\n",
    "#             print('param2: ',  param2)\n",
    "            if param2 == 'losses':\n",
    "                continue\n",
    "\n",
    "#             ### YELLOWBRICK VALIDATION CURVE    \n",
    "#             if param2 not in col_rid:  # se for um parametro do data5, prosseguir\n",
    "# #                 print('param2_: ',  param2)\n",
    "#                 param_range = np.sort(list(set(np.random.choice(vals2, 20)))) # sampleia 20 pontos do total. Sem repeticao\n",
    "#                 viz = ValidationCurve(clf_2, param_name=param2, param_range=param_range, scoring='accuracy', n_jobs=-1)\n",
    "#                 viz.fit(X_train, y_train)\n",
    "#                 viz.poof()\n",
    "#                 plt.show()\n",
    "\n",
    "            ### HYPEROPT VALIDATION CURVE - LOSS FUNCTION\n",
    "            data_temp = None # used exclusively on HERE \n",
    "            data_temp = data4[[ param2, 'losses']].dropna().copy()\n",
    "#             print('param2__: ',  param2)\n",
    "            x = data_temp[param2]\n",
    "            y = data_temp.losses\n",
    "            plt.scatter(x,y, alpha=0.2, c=y, cmap='RdBu')\n",
    "            plt.xlabel(param2)\n",
    "            plt.ylabel('Score')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "   \n",
    "    def plot_models_corr(self, chosen=None):\n",
    "        if chosen:\n",
    "            corr = pd.DataFrame({k:v['y_pred'] for k,v in self.evaluations_all.items() if k in chosen}).corr()\n",
    "        else:\n",
    "            corr = pd.DataFrame({k:v['y_pred'] for k,v in self.evaluations_all.items()}).corr()\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(14,10)); \n",
    "        sns.heatmap(corr, annot=True, cmap=\"YlGnBu\", ax=ax)    \n",
    "\n",
    "        \n",
    "        \n",
    "    def plot_Classification_Report(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "\n",
    "        for name in list_members:\n",
    "            path = '../pkl/c_reprt_'+ name + '.png'\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "            \n",
    "                display(Image(path))\n",
    "            else:\n",
    "\n",
    "                pipe = self.evaluations_all[name]['pipe_best']\n",
    "                visualizer = ClassificationReport(pipe, support=True)\n",
    "\n",
    "                visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "                visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "                visualizer.poof(outpath=path)\n",
    "                plt.clf()\n",
    "                print('saving c_reprt_ image for',  name)       \n",
    "            \n",
    "\n",
    "    def plot_Confusion_Matrix(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "    \n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "        for name in list_members:\n",
    "            path = '../pkl/c_matrx_'+ name + '.png'\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                display(Image(path))\n",
    "\n",
    "            else:\n",
    "                pipe = self.evaluations_all[name]['pipe_best']\n",
    "                visualizer = ConfusionMatrix(pipe, percent=True)\n",
    "                visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "                visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "                visualizer.poof(outpath=path) \n",
    "                plt.clf()\n",
    "                print('saving c_matrx_ image for',  name)       \n",
    "            \n",
    "\n",
    "    def plot_ROC_AUC(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "\n",
    "        for name in list_members:\n",
    "            print(name)\n",
    "            \n",
    "            path = '../pkl/roc_auc_'+ name + '.png'\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                display(Image(path))\n",
    "\n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = ROCAUC(clf)\n",
    "                visualizer.fit(X_train, y_train)\n",
    "                print(type(X_test), type(X_test))\n",
    "                visualizer.score(X_test, np.array(y_test))  \n",
    "                visualizer.poof(outpath = path)  \n",
    "                plt.clf()\n",
    "                print('saving roc_auc_ image for',  name)       \n",
    "     \n",
    "    \n",
    "    # isso eh igual ao histograma do y ?\n",
    "    def plot_Class_Balance(self, chosen=None):\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "\n",
    "        for name in list_members:\n",
    "            clf = self.evaluations_all[name]['best_estimator']\n",
    "            visualizer = ClassBalance(clf)\n",
    "\n",
    "            visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "            visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "            visualizer.poof()             # Draw/show/poof the data\n",
    "\n",
    "\n",
    "    # NOT WORKING\n",
    "    def plot_Class_Prediction_Error(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "\n",
    "        for name in filtered:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                image = mpimg.imread('../pkl/c_pred_err_'+ name + '.png')\n",
    "                plt.clf()\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = ClassPredictionError(clf)\n",
    "                visualizer.fit(X_train, y_train)  \n",
    "                visualizer.score(X_test, y_test)  \n",
    "                visualizer.poof(outpath = '../pkl/c_pred_err_'+ name + '.png')  \n",
    "                plt.clf()\n",
    "                print('saving c_pred_err_ image for',  name)       \n",
    "     \n",
    "\n",
    "    def plot_Discrimination_Threshold(self,load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "\n",
    "        for name in list_members:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                path = '../pkl/discri_thres_'+ name + '.png'\n",
    "                display(Image(path))\n",
    "                \n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = DiscriminationThreshold(clf)\n",
    "                visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "                visualizer.poof(outpath = '../pkl/disc_thres_'+ name + '.png')  \n",
    "                plt.clf()\n",
    "                print('saving disc_thres_ image for',  name)       \n",
    "    \n",
    "    # NOT WORKING\n",
    "    # tentar juntar com PCA ja que o plot é 2D \n",
    "    def plot_Decision_Boundaries(self,load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "\n",
    "        for name in list_members:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                path = '../pkl/dec_bound_'+ name + '.png'\n",
    "                display(Image(path))\n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = DecisionBoundariesVisualizer(clf)\n",
    "                visualizer.fit(X_train, y_train)  \n",
    "                visualizer.poof(outpath = '../pkl/dec_bound_'+ name + '.png')  \n",
    "                plt.clf()\n",
    "                print('saving dec_bound_ image for',  name)       \n",
    "     \n",
    "# ESSA MERDA AINDA TEM USO ?\n",
    "    def plot_Viz_pipe(self, pipe, name, load_saved=True):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "            'pca_proj_', 'manif_' \n",
    "            plot PCA PROJECTION or Manifold as last pipeline step\n",
    "        '''\n",
    "        if load_saved == True:\n",
    "            print('opening the image',  name)\n",
    "            path = '../pkl/dec_bound_'+ name + '.png'\n",
    "            display(Image(path))\n",
    "\n",
    "\n",
    "            display(Image.open('../pkl/'+ name + '.png'))\n",
    "        else:\n",
    "            pipe.fit_transform(X_train, y_train.values.ravel())\n",
    "            pipe.named_steps['viz'].poof(outpath='../pkl/'+ name + '.png')\n",
    "            print('saving viz image for',  name)       \n",
    "\n",
    "\n",
    "\n",
    "    def plot_Feature_Importances(self, X_train, y_train, chosen=None, col_filter=None):\n",
    "        '''sklearn`s '''\n",
    "        if col_filter:    # to filter by  \n",
    "            X_train = X_train[col_filter]\n",
    "            \n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "\n",
    "        for name in list_members:\n",
    "            clf = self.evaluations_all[name]['instance']\n",
    "            visualizer = FeatureImportances(clf)\n",
    "\n",
    "            visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "            visualizer.poof()             # Draw/show/poof the data\n",
    "\n",
    "            \n",
    "    # falta ordenar e anotar os valores          \n",
    "    def plot_feature_importance_mapper(self, pipe, mapper, est):\n",
    "        '''est is a string of the primitive estimator`s name referenced in the Pipeline steps.\n",
    "        ex: 'DT' ,'KNN'  '''\n",
    "        y = mapper.transformed_names_\n",
    "        x = pipe.named_steps[est].feature_importances_\n",
    "        sns.barplot(x, y)\n",
    "        \n",
    "                \n",
    "    def explain_weights(self, name):\n",
    "        '''ELI 5'''\n",
    "        est = Super_Analytica.evaluations_all[name]['instance']\n",
    "        vec = map_tfidf.features[0][1][1]\n",
    "        return eli5.show_weights(est, vec=vec)\n",
    "\n",
    "    \n",
    "    \n",
    "    def explain_prediction(self, name):\n",
    "        '''ELI 5'''\n",
    "        est = Super_Analytica.evaluations_all[name]['instance']\n",
    "        vec = map_tfidf.features[0][1][1]\n",
    "        return eli5.show_prediction(est, X_train.iloc[1,:][0] , vec=vec)\n",
    "     \n",
    "        \n",
    "    def get_feature_names(self, my_name, pipe_steps):\n",
    "        ''' tekes two strings and return the feature names '''\n",
    "        p = Super_Analytica.evaluations_all[my_name]['best_estimator'] # acessa cloud e pega o best estimator\n",
    "    \n",
    "        return p.get_params()[pipe_steps].get_feature_names()\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "##############     EVALUATION SECTION     ###############\n",
    "    \n",
    "    def evaluate_CV(self, pipe, chosen, sufix='', pkl_W=False, vectorize_first=False):\n",
    "        ''' iterates over the dict of models clf and not over Evaluations_all\n",
    "            chosen must be a list of elements from clf. \n",
    "            sufix must be a string. KNN_1234 --> KNN_sufix\n",
    "            the input pipe must not have the last step. It will be placed dynamically by pipe.append.\n",
    "            and applied to all chosen.\n",
    "        '''\n",
    "        assert set(chosen).issubset(set(clf.keys())) # all elements in chosen are in clf\n",
    "      \n",
    "        for name in chosen:\n",
    "            start = timer()\n",
    "#             pipe_copy = None\n",
    "#             pipe_copy = clone(pipe)\n",
    "#             pipe_copy.steps.append([name, clf[name]])\n",
    "\n",
    "            pipe_full = clone(pipe)\n",
    "            pipe_full.steps.append([name, clf[name]])\n",
    "        \n",
    "            if vectorize_first==True:\n",
    "                try:   # Depending on the pipeline`s parameters both X_train and y_train must be fitted\n",
    "                    X_train_ = pipe.fit_transform(X_train)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    X_train_ = pipe.fit_transform(X_train, y_train)   \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                X_test_ = pipe.transform(X_test)\n",
    "                obj_1 = clf[name]\n",
    "\n",
    "            else:\n",
    "                X_train_ = X_train\n",
    "                X_test_ = X_test\n",
    "                obj_1 = pipe_full\n",
    "            \n",
    "            y_pred = cross_val_predict(obj_1, X_train_, y_train, cv=kfold)\n",
    "            acc = cross_val_score(obj_1, X_train_, y_train, scoring='roc_auc', cv=kfold).mean() \n",
    "            end = timer()\n",
    "            minutes_elapsed = (end - start)//60\n",
    "#             name = name.split('_')[0] + sufix #tem q primeiro limpar name e depois aplicar o sufix\n",
    "            name = name + sufix \n",
    "            Super_Analytica.evaluations_all[name] = {'score': acc, 'best_estimator': clf[name], \n",
    "                                                             'param_grid': None, 'y_pred': y_pred, \n",
    "                                                             'param_pipe': pipe_full, 'instance':None,\n",
    "                                                             'best_params' : None, 'T': None, 'minutes': minutes_elapsed }             \n",
    "            if  pkl_W:\n",
    "                D = {}\n",
    "                D[name]= Super_Analytica.evaluations_all[name]\n",
    "                joblib.dump(D, '../pkl/clf_'+ name + '.pkl')        \n",
    "                print(name, '  Cross Validation and Pickled Done in ', minutes_elapsed, ' minutes', ' /-->acc :', acc , '\\n') \n",
    "            else:\n",
    "                print(name, '  Cross Validation Done in ', minutes_elapsed, ' minutes', ' /-->acc :', acc , '\\n') \n",
    "         \n",
    "        \n",
    "    def translate(self, my_name):   #plot_validation_curve\n",
    "            # in: data4 , param_grid\n",
    "        # out: translated data4 \n",
    "        # varre todo o param_grid a procura dos param X e faz a traducao em data4\n",
    "    #     super1.evaluations_all['DT_7']['param_grid']\n",
    "\n",
    "        # translates FROM STEPS(len) TO the chosen range of VALUES\n",
    "        data4 = Super_Analytica.make_data4(self, my_name)\n",
    "        P = Super_Analytica.evaluations_all[my_name]['param_grid']\n",
    "        hyperopt_list = [hyperopt.pyll_utils.str(v) for k,v in P.items()] # abrindo o hyperopt.utils numa lista of long strings. one string for each param\n",
    "    #     print(hyperopt_list)\n",
    "\n",
    "        def helper_split_string(s): # breaks up the long string and remove ''. returns a list of strings\n",
    "            ls = [i for i in s.split(' ')] \n",
    "            for i in range(len(ls)):\n",
    "                if '' in ls:\n",
    "                    ls.remove('')\n",
    "            return ls\n",
    "\n",
    "        hyperopt_list2 = list(map(helper_split_string, hyperopt_list))   # lista hyperopt.utils ready to be analysed for patterns\n",
    "#         print(hyperopt_list2)\n",
    "\n",
    "        pat = r'\\w+\\{(\\w+)\\}\\W+[n3]'\n",
    "#         pat2 = r'\\w+\\{(\\d+)\\}'\n",
    "        pat2 = r'\\{([0-9]*[.]?[0-9]+)\\}'\n",
    "\n",
    "\n",
    "        D = {}     # dict to store the whole range of real values\n",
    "        for L in hyperopt_list2:  # procurar pelo param que comeca com X e retrieve values\n",
    "            if len(L) == 1:\n",
    "                break\n",
    "            else:\n",
    "                w = L[3]  # captura o 4 termo\n",
    "                if re.search(pat, w): # checa se dentro dos {} tem uma palavra\n",
    "            #         print('yes')\n",
    "                    m = re.search(pat, w).groups()[0] # var com o termo capturado, que é um key de um dos dicts  de param_1\n",
    "                    if m.startswith('zz_'):  # Checa se o termo capturado inicia com 'zz_'. CAso sim, processa adiante.\n",
    "#                         print(m)\n",
    "#                         print(L[6:])\n",
    "                        D[m] = [re.search(pat2, i).groups()[0] for i in L[6:]] # varre todos os mebros apos o 6 membro capturando o integer via o pat2. coleta todo o range de valores para cada param. Cada key é um param com prefixo 'X'.\n",
    "            #             print(D[m])\n",
    "                        data4[m] = data4[m].map(lambda x: D[m][int(x)]).astype('float64') # map each value x of the given column data4[m] with the index x in the respect dict D[m]. x represent the step     update the column in data4. Columns have 'X' prefix\n",
    "        data4.columns = data4.columns.map(lambda x: x.strip('X') if x.startswith('X') else x) # renomeia as colunas para nao ter X\n",
    "        data4 = data4.reindex(sorted(data4.columns), axis=1) # order the columns afabetically\n",
    "#         print('end translate')\n",
    "        ### END DATA PREP\n",
    "        return data4\n",
    "    \n",
    "    \n",
    "    def make_data4(self, my_name):   # plot_validation_curve\n",
    "        '''ingest the Trials dict, clean it and return a Data Frame with the sampled values to be updated ahead.'''\n",
    "\n",
    "    #     data3 = super1.evaluations_all['DT_7']['T']\n",
    "        data3 = Super_Analytica.evaluations_all[my_name]['T']\n",
    "    #     data3\n",
    "\n",
    "        d = [i['misc']['vals'] for i in  data3.trials]\n",
    "    #     d[:3]\n",
    "\n",
    "        L = []\n",
    "        for i in  d:\n",
    "            d_transformed = {k:v[0] if v !=[] else v  for k,v in i.items()} # retirando os valores de dentro das lista.\n",
    "            L.append(d_transformed)\n",
    "    #     L[0]\n",
    "\n",
    "        param_list = [list(i['misc']['vals'].keys())  for i in data3.trials[:1]][0]\n",
    "        param_list_noChoices =  [i for i in param_list if i != 'choices' ]\n",
    "    #     param_list\n",
    "    #     param_list_noChoices\n",
    "\n",
    "        data4 = pd.DataFrame(L)\n",
    "        data4['losses'] = [1-i for i in data3.losses()] # invertendo de losses --> score\n",
    "        data4 = data4.applymap(lambda x: np.nan if x == [] else x) # transforma os [] em NaN - para qnd houver choice boladao\n",
    "        return data4 # este dataset contem as colunas'X' que deverao ser traduzidas posteriormente\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T03:54:32.442429Z",
     "start_time": "2018-05-19T03:54:32.411577Z"
    },
    "code_folding": [
     13,
     61
    ]
   },
   "outputs": [],
   "source": [
    "class Analytica_Bolada(object):\n",
    "    \n",
    "#     def __init__(self, models, my_name):\n",
    "    def __init__(self, instance, param_grid, my_name):\n",
    "                \n",
    "#         Super_Analytica.__init__(self,  my_name)\n",
    "#         super().__init__(  my_name)\n",
    "        self.my_name = my_name\n",
    "        self.instance = instance\n",
    "        self.param_grid = param_grid\n",
    "#         self.models = models\n",
    "#         self.models = dict(self.my_name=(instance, param_grid))\n",
    "#         print(self.models)\n",
    "        Super_Analytica.insert_member(self)\n",
    "        \n",
    "        \n",
    "\n",
    "#     def plot_learning_curve(self):\n",
    "#         clf = Super_Analytica.evaluations_all[self.my_name]['best_estimator']\n",
    "# #         print(clf)\n",
    "#         Super_Analytica.helper_plot_learning_curve(self, clf, self.my_name)\n",
    "        \n",
    "\n",
    "        \n",
    "#     def plot_validation_curve(self):\n",
    "#         Super_Analytica.helper_plot_validation_curve(self, self.my_name)\n",
    "   \n",
    "#     n_jobs=-1 did not wor in pipws with truncatedSVD\n",
    "    # assume pipe declarado stardard, e nao como no HP \n",
    "    def evaluate_CV(self, pipe,  pkl_W=False):\n",
    "        \n",
    "        start= timer()\n",
    "        y_pred = cross_val_predict(pipe, X_train, y_train, cv=kfold)  \n",
    "        acc = cross_val_score(pipe, X_train, y_train, cv=kfold).mean()\n",
    "        print('acc :', acc)\n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': pipe, \n",
    "                                                         'param_grid': None, 'y_pred': y_pred, \n",
    "                                                         'param_pipe': pipe, 'instance':self.instance,\n",
    "                                                         'best_params' : None, 'T': None, 'minutes': minutes_elapsed} \n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name] = Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl')        \n",
    "            print(self.my_name, '  Cross Validation and Pickled Done in ', minutes_elapsed, ' minutes', '\\n') \n",
    "        else:\n",
    "            print(self.my_name, '  Cross Validation Done in ', minutes_elapsed, ' minutes', '\\n') \n",
    "\n",
    "       \n",
    "\n",
    "    # assume pipe declarado stardard, e nao como no HP \n",
    "    def evaluate_GS(self, param_pipe=None):\n",
    "        \n",
    "        if param_pipe == None:               # sem pipeline at instantiation\n",
    "#             pipe = Pipeline(param_pipe) \n",
    "            est = self.instance\n",
    "\n",
    "        else:                             # com pipeline\n",
    "\n",
    "#             param_pipe.append((key,instance))  # append the classifier at the last position after the transformers\n",
    "            est = param_pipe \n",
    "            print(est)\n",
    "\n",
    "\n",
    "        \n",
    "        clf = GridSearchCV(est, param_grid=self.param_grid, cv=kfold)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': clf.best_score_, 'best_estimator': clf.best_estimator_, \\\n",
    "                                                         'param_grid': self.param_grid, 'y_pred': y_pred, \\\n",
    "                                                         'param_pipe': None, 'instance': self.instance,\\\n",
    "                                                         'best_params': clf.best_params_, 'T': None } \n",
    "\n",
    "        print(self.my_name, '  GridSearch Evaluation is Done' )\n",
    "       \n",
    "    \n",
    "    \n",
    "    # pipe not implemented\n",
    "    def evaluate_R_GS(self, param_pipe=None):\n",
    "        clf = RandomizedSearchCV(self.instance, param_grid=self.param_grid, cv=kfold)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': clf.best_score_, 'best_estimator': clf.best_estimator_, \\\n",
    "                                                         'param_grid': self.param_grid, 'y_pred': y_pred, \\\n",
    "                                                         'param_pipe': None, 'instance': self.instance,\\\n",
    "                                                         'best_params': clf.best_params_, 'T': None } \n",
    "\n",
    "        print(self.my_name, '  Randomized GridSearch Evaluation is Done' )\n",
    "\n",
    "\n",
    "    def evaluate_HP2(self, param_pipe, feature_space=None, pkl_W=False):  # sub\n",
    "        '''\n",
    "        so funciona com Pipeline e sendo declarado conforme o padrao. Nao é que nem no HP\n",
    "        '''\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "        X_train_internal = X_train\n",
    "        X_test_internal = X_test\n",
    "\n",
    "        key = self.my_name.split('_')[0] # to use in pipe. From KNN_2 -> KNN. Must mach param_grid prefix\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "        pipe = param_pipe\n",
    "\n",
    "\n",
    "        def objective(params):\n",
    "            pipe.set_params(**params)  # diferente do caso sem pipe\n",
    "            score = cross_val_score(pipe, X_train_internal, y_train, scoring='accuracy', cv=kfold) \n",
    "            return 1 - score.mean()\n",
    "        \n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) # is the translation of best. Real Values, not steps.\n",
    "        pipe.set_params(**best_params)\n",
    "        clf = pipe.fit(X_train_internal, y_train) \n",
    "\n",
    "        y_pred = clf.predict(X_test_internal)  # predict over unseen data\n",
    "        # nao seria mais barato usar accuracy score para nao ter que rodar predict de novo \n",
    "        acc = pipe.score(X_test_internal, y_test) # DUVIDA poderia usar ROC score por exemplo ?\n",
    "        print('acc :', acc)\n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': clf, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "                                 'param_pipe': param_pipe, 'instance':instance,\\\n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "                \n",
    "        \n",
    "        D = {}\n",
    "        D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "\n",
    "        if  pkl_W:\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, '  pickled to disk' ) \n",
    "        print(self.my_name, '  HP2 done in ', minutes_elapsed, ' minutes') \n",
    "        \n",
    "        \n",
    " \n",
    "    def evaluate_HP3(self, pipe, pkl_W=False): \n",
    "        '''\n",
    "        aceita pipeline sem o step final. pipe.append()\n",
    "        '''\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "\n",
    "#         key = self.my_name.split('_')[0] # to use in pipe. From KNN_2 -> KNN. Must mach param_grid prefix\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "\n",
    "        pipe_copy = None\n",
    "        pipe_copy = clone(pipe)\n",
    "        pipe_copy.steps.append([instance, clf[instance]])\n",
    "\n",
    "        def objective(params):\n",
    "            pipe_copy.set_params(**params)  # diferente do caso sem pipe\n",
    "            score = cross_val_score(pipe_copy, X_train, y_train, scoring='roc_auc', cv=kfold) # n_jobs=-1) \n",
    "            print('cv score:', score.mean())\n",
    "            return 1 - score.mean()\n",
    "        \n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) # is the translation of best. Real Values, not steps.\n",
    "        pipe_copy.set_params(**best_params)\n",
    "        clf_ = pipe_copy.fit(X_train, y_train) \n",
    "\n",
    "#         y_pred = clf_.predict(X_test)  # predict over unseen data\n",
    "        y_pred = clf_.predict_proba(X_test)[:, 1]\n",
    "#         acc = pipe_copy.score(X_test, y_test) # DUVIDA poderia usar ROC score por exemplo ?\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('acc :', acc)\n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': clf_, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "                                 'param_pipe': pipe_copy, 'instance':instance,\\\n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "                \n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP3 and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP3 done in ', minutes_elapsed, ' minutes') \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_HP3_vect(self, pipe, vectorize_first=False, pkl_W=False): \n",
    "        '''\n",
    "        Essa funcao realiza parameter tunning via cv e aplica best_model em todo dataset treino.\n",
    "        O pipeline 'pipe' nao deve conter um classificador como ultimo elemento.\n",
    "        Este sera adicionado via pipe.apend(model) caso vectorize_first=False\n",
    "        Se vectorize_first=True, tanto X_train como X_test serao pre-processados de antemao pelos parametros do\n",
    "        pipeline (que nao contem classificador)\n",
    "        '''\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "        \n",
    "#         key = self.my_name.split('_')[0] # to use in pipe. From KNN_2 -> KNN. Must mach param_grid prefix\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "\n",
    "        pipe_full = clone(pipe)\n",
    "        pipe_full.steps.append([instance, clf[instance]])\n",
    "\n",
    "        \n",
    "        if vectorize_first==True:\n",
    "            try:   # Depending on the pipeline`s parameters both X_train and y_train must be fitted\n",
    "                X_train_ = pipe.fit_transform(X_train)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                X_train_ = pipe.fit_transform(X_train, y_train)   \n",
    "            except:\n",
    "                pass\n",
    "            X_test_ = pipe.transform(X_test)\n",
    "            obj_1 = clf[instance]\n",
    "            \n",
    "        else:\n",
    "            X_train_ = X_train\n",
    "            X_test_ = X_test\n",
    "            obj_1 = pipe_full\n",
    "            \n",
    "        \n",
    "        def objective(params):\n",
    "            obj_1.set_params(**params)  # diferente do caso sem pipe\n",
    "            score = cross_val_score(obj_1, X_train_, y_train, scoring='roc_auc', cv=kfold) # n_jobs=-1) \n",
    "            return 1 - score.mean()\n",
    "            mdl = obj_1\n",
    "            mdl.fit\n",
    "            \n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=20, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) # is the translation of best. Real Values, not steps.\n",
    "        obj_1.set_params(**best_params)\n",
    "        clf_ = obj_1.fit(X_train_, y_train) \n",
    "\n",
    "#         y_pred = clf_.predict(X_test)  # predict over unseen data\n",
    "        y_pred = clf_.predict_proba(X_test_)[:, 1]\n",
    "#         acc = pipe_copy.score(X_test, y_test) # DUVIDA poderia usar ROC score por exemplo ?\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('acc :', acc)\n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': clf_, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "                                 'param_pipe': pipe_full, 'instance':instance,\\\n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "                \n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP3 and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP3 done in ', minutes_elapsed, ' minutes') \n",
    "        \n",
    "\n",
    "        \n",
    "    def evaluate_HP3_vect_val(self, pipe, vectorize_first=False, pkl_W=False): \n",
    "        '''\n",
    "        Essa funcao realiza parameter tunning via cv e aplica best_model em todo dataset treino.\n",
    "        O pipeline 'pipe' nao deve conter um classificador como ultimo elemento.\n",
    "        Este sera adicionado via pipe.apend(model) caso vectorize_first=False\n",
    "        Se vectorize_first=True, tanto X_train como X_test serao pre-processados de antemao pelos parametros do\n",
    "        pipeline (que nao contem classificador)\n",
    "        Se vectorize_first=True o param grid nao deve ter double undescore, 'LGR__C' --> 'C'\n",
    "        '''\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "        \n",
    "#         key = self.my_name.split('_')[0] # to use in pipe. From KNN_2 -> KNN. Must mach param_grid prefix\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "\n",
    "        pipe_full = clone(pipe)\n",
    "        pipe_full.steps.append([instance, clf[instance]])\n",
    "\n",
    "        \n",
    "        if vectorize_first==True:\n",
    "            try:   # Depending on the pipeline`s parameters both X_train and y_train must be fitted\n",
    "                X_train_ = pipe.fit_transform(X_train)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                X_train_ = pipe.fit_transform(X_train, y_train)  # condicional aqui. nem sempre precisa fitar com y_train. da maneira atual esta obrigaotiro \n",
    "            except:\n",
    "                pass\n",
    "            X_test_ = pipe.transform(X_test)\n",
    "            X_val_ = pipe.transform(X_val)\n",
    "            obj_1 = clf[instance]\n",
    "            \n",
    "        else:\n",
    "            X_train_ = X_train\n",
    "            X_test_ = X_test\n",
    "            obj_1 = pipe_full\n",
    "            \n",
    "        \n",
    "        def objective(params):\n",
    "            obj_1.set_params(**params)  # diferente do caso sem pipe\n",
    "            mdl = obj_1.fit(X_train_, y_train) # n_jobs=-1) \n",
    "            y_pred = mdl.predict(X_val_)\n",
    "            score = roc_auc_score(y_val, y_pred)\n",
    "            print('val score: ', score)\n",
    "            return 1 - score\n",
    "        \n",
    "            \n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) # is the translation of best. Real Values, not steps.\n",
    "        obj_1.set_params(**best_params)\n",
    "        clf_ = obj_1.fit(X_train_, y_train) \n",
    "\n",
    "#         y_pred = clf_.predict(X_test)  # predict over unseen data\n",
    "        y_pred = clf_.predict_proba(X_test_)[:, 1]\n",
    "#         acc = pipe_copy.score(X_test, y_test) # DUVIDA poderia usar ROC score por exemplo ?\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('acc :', acc)\n",
    "        \n",
    "        pipe_best = clone(pipe)\n",
    "        pipe_best.steps.append([self.name, clf[instance].set_params(**best_params)]) \n",
    "        \n",
    "\n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': clf_, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "                                 'pipe_best': pipe_best, 'instance':instance,\\\n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "                \n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP3_vect_val and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP3_vect_val done in ', minutes_elapsed, ' minutes') \n",
    "\n",
    "\n",
    "            \n",
    "    def evaluate_Parfit(self, pipe, pkl_W=False): \n",
    "        '''\n",
    "        Essa funcao realiza parameter tunning via validation set e aplica best_model em todo dataset treino.\n",
    "        O pipeline 'pipe' nao deve conter um classificador como ultimo elemento.\n",
    "        X_train, X_val, X_test serao vetorizados de acordo com o pipeline pipe.\n",
    "        O praram_grid deve conter ParameterGrid({})\n",
    "            \n",
    "        '''\n",
    "        start= timer()\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid        \n",
    "\n",
    "\n",
    "#         try:   # Depending on the pipeline`s parameters both X_train and y_train must be fitted\n",
    "#             X_train_ = pipe.fit_transform(X_train)\n",
    "#         except:\n",
    "        X_train_ = pipe.fit_transform(X_train, y_train)   \n",
    "#         try:\n",
    "#             X_train_ = pipe.fit_transform(X_train, y_train)   \n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "        X_val_ = pipe.transform(X_val)\n",
    "        X_test_ = pipe.transform(X_test)\n",
    "        \n",
    "        best_model, best_score, all_models, all_scores = pf.bestFit(clf[instance], \\\n",
    "                                                                     \n",
    "            param_grid, X_train_, y_train, X_val_, y_val,\\\n",
    "            metric=roc_auc_score, scoreLabel='AUC')\n",
    "#         print(best_model)\n",
    "        \n",
    "        mdl = best_model.fit(X_train_, y_train) \n",
    "        y_pred = mdl.predict_proba(X_test_)[:, 1]\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('acc :', acc)\n",
    "\n",
    "        \n",
    "        pipe_best = clone(pipe)\n",
    "        pipe_best.steps.append([self.my_name, best_model])\n",
    "\n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': best_model , \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "                                 'pipe_best': pipe_best, 'instance':instance,\\\n",
    "                                 'best_params' : best_model, 'T': None, 'minutes': minutes_elapsed} \n",
    "          \n",
    "        print('start pickling')\n",
    "        if pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl')     \n",
    "            print(self.my_name, ' Parfit and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  Parfit done in ', minutes_elapsed, ' minutes') \n",
    "        \n",
    "    \n",
    "         \n",
    "    def evaluate_HP_lGBM(self, pipe, pkl_W=False): \n",
    "        '''\n",
    "        This function uses HyperOpt to tune LightGHM params on train and validation set. Than it refits the best model on entire trainset and score on testset.\n",
    "        '''\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "\n",
    "\n",
    "        def objective(params):\n",
    "            # Make sure parameters that need to be integers are integers\n",
    "            for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "                params[parameter_name] = int(params[parameter_name])\n",
    "\n",
    "            model = clf[instance].set_params(**params)\n",
    "            model.fit(train_feature_matrix, y_train, eval_set=(val_feature_matrix, y_val),\\\n",
    "                      eval_names=['val_feature_matrix', 'y_val'], eval_metric='auc', early_stopping_rounds=10, verbose=False)             \n",
    "            best_score = model.best_score_['val_feature_matrix']['auc']\n",
    "            print('val score: ', best_score)\n",
    "            return 1 - best_score\n",
    "\n",
    "\n",
    "        train_feature_matrix = pipe.fit_transform(X_train) # pipe descalco sem o ultimo\n",
    "        data_train = lgb.Dataset(train_feature_matrix, label=list(y_train.values.ravel()))\n",
    "\n",
    "        test_feature_matrix = pipe.fit_transform(X_test) # pipe descalco sem o ultimo\n",
    "        data_test = lgb.Dataset(test_feature_matrix, label=list(y_test.values.ravel()))\n",
    "        \n",
    "        val_feature_matrix = pipe.fit_transform(X_val) \n",
    "\n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=3, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) \n",
    "\n",
    "        for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "                best_params[parameter_name] = int(best_params[parameter_name])\n",
    "\n",
    "        best_model = clf[instance].set_params(**best_params)\n",
    "        best_model.fit(train_feature_matrix, y_train.values.ravel())\n",
    "        y_pred = best_model.predict_proba(test_feature_matrix)[:,1]\n",
    "#         y_pred = best_model.predict(test_feature_matrix)\n",
    "#         print(y_pred)\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('test score :', acc)\n",
    "        \n",
    "        pipe_best = clone(pipe)\n",
    "        pipe_best.steps.append([self.my_name, best_model])\n",
    "#         print('pipe_best :', pipe_best.steps)\n",
    "        \n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': best_model, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \n",
    "                                 'pipe_best': pipe_best, 'instance':instance, # é somente uma string, nao é a instance em si\\ \n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "\n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP_lightGBM and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP_lightGBM done in ', minutes_elapsed, ' minutes') \n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_HP_lGBM_choice(self, pipe, pkl_W=False): \n",
    "        '''\n",
    "        aceita pipeline sem o step final. pipe.append()\n",
    "        '''\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "\n",
    "#         pipe_copy = None\n",
    "#         pipe_copy = clone(pipe)\n",
    "#         pipe_copy.steps.append([instance, clf[instance]])\n",
    "\n",
    "        def objective(params):\n",
    "\n",
    "            # Retrieve the subsample if present otherwise set to 1.0\n",
    "            subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "            # Extract the boosting type\n",
    "            params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "            params['subsample'] = subsample\n",
    "\n",
    "            # Make sure parameters that need to be integers are integers\n",
    "            for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "                params[parameter_name] = int(params[parameter_name])\n",
    "\n",
    "        #     print(params)\n",
    "            model = clf[instance].set_params(**params)\n",
    "            model.fit(train_feature_matrix, y_train, eval_set=(val_feature_matrix, y_val), eval_names=[\n",
    "                      'val_feature_matrix', 'y_val'], eval_metric='auc', early_stopping_rounds=10, verbose=False)\n",
    "            best_score = model.best_score_['val_feature_matrix']['auc']\n",
    "            print('val score: ', best_score)\n",
    "            return 1 - best_score\n",
    "\n",
    "\n",
    "\n",
    "        train_feature_matrix = pipe.fit_transform(X_train) # pipe descalco sem o ultimo\n",
    "        data_train = lgb.Dataset(train_feature_matrix, label=list(y_train.values.ravel()))\n",
    "\n",
    "        test_feature_matrix = pipe.fit_transform(X_test) # pipe descalco sem o ultimo\n",
    "        data_test = lgb.Dataset(test_feature_matrix, label=list(y_test.values.ravel()))\n",
    "        \n",
    "        val_feature_matrix = pipe.fit_transform(X_val) \n",
    "\n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=5, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) \n",
    "\n",
    "        # Retrieve the subsample if present otherwise set to 1.0\n",
    "        subsample = best_params['boosting_type'].get('subsample', 1.0)\n",
    "        # Extract the boosting type\n",
    "        best_params['boosting_type'] = best_params['boosting_type']['boosting_type']\n",
    "        best_params['subsample'] = subsample\n",
    "        # Make sure parameters that need to be integers are integers\n",
    "        for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "            best_params[parameter_name] = int(best_params[parameter_name])\n",
    "\n",
    "\n",
    "        best_model = clf[instance].set_params(**best_params)\n",
    "        best_model.fit(train_feature_matrix, y_train.values.ravel())\n",
    "        y_pred = best_model.predict_proba(test_feature_matrix)[:,1]\n",
    "#         y_pred = best_model.predict(test_feature_matrix)\n",
    "#         print(y_pred)\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('test score :', acc)\n",
    "        \n",
    "        pipe_best = clone(pipe)\n",
    "        pipe_best.steps.append([self.my_name, best_model])\n",
    "\n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': best_model, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \n",
    "                                 'pipe_best': pipe_best, 'instance':instance, # é somente uma string, nao é a instance em si\\ \n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "\n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP_lightGBM_choice and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP_lightGBM_choice done in ', minutes_elapsed, ' minutes') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineRFE(Pipeline):\n",
    "\n",
    "    def fit(self, X_train, y_train=None, **fit_params):\n",
    "        super(PipelineRFE, self).fit(X_train, y_train, **fit_params)\n",
    "        self.feature_importances_ = self.steps[-1][-1].feature_importances_\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best method to find if fitted\n",
    "# for model in L:\n",
    "#     try:\n",
    "#         model.predict(X_test)\n",
    "#         print(model, 'is FITTED \\n')\n",
    "#     except ValueError:\n",
    "#         print(model, 'NOT FITTED \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_models(s):\n",
    "    '''look in md_all for the models that starts with s return it in a list os strings '''\n",
    "    all_names = [name for L in md_all for name in L] # from list of lists to a single list\n",
    "    return [name for name in all_names if name.startswith(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "333px",
    "left": "1376px",
    "top": "505px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
