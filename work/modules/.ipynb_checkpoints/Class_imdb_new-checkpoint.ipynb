{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T03:54:32.442429Z",
     "start_time": "2018-05-19T03:54:32.411577Z"
    },
    "code_folding": [
     13,
     61
    ]
   },
   "outputs": [],
   "source": [
    "class Analytica_2(object):\n",
    "    WHO_LIST = []    #. classe Super nao fica registrada na WHOLIST.\n",
    "    evaluations_all = {}\n",
    "    \n",
    "    def __init__(self, pipe):\n",
    "        '''na declaracao da classe, o obj fica relacionado ao pipe podendo ser usado por qq funcao '''\n",
    "        self.pipe = pipe    \n",
    "#         self.sufix = sufix\n",
    "# #         Analytica.insert_member(self)\n",
    "#         Analytica.make_data(self)\n",
    "        \n",
    "        \n",
    "\n",
    "    def insert_member(self):   # only used in the Sub init\n",
    "#         Super_Analytica.WHO_LIST.append(self.my_name)  ## APAGAR DEPOIS\n",
    "        if not self.my_name in Analytica_2.WHO_LIST:\n",
    "            Analytica_2.WHO_LIST.append(self.my_name)\n",
    "            print(self.my_name, '  Instance Created')\n",
    "            print('\\n')\n",
    "        else:\n",
    "            raise ValueError(self.my_name, ' This instance is already in WHOLIST. Or pick another name.')\n",
    "            \n",
    "    def make_data(self, sufix, pkl_W=False):\n",
    "#         self.pipe = pipe \n",
    "        print('Vectorizing Data')\n",
    "        self.X_train_ = self.pipe.fit_transform(X_train, y_train)  \n",
    "        self.X_test_ = self.pipe.transform(X_test)\n",
    "        self.X_val_ = self.pipe.transform(X_val)\n",
    "        \n",
    "        print(self.X_train_.shape)\n",
    "        print(self.X_test_.shape)\n",
    "        print(self.X_val_.shape)\n",
    "        \n",
    "        if pkl_W:\n",
    "            filenames = ['X_train_', 'X_test_', 'X_val_']\n",
    "            for name, data in zip(filenames, [self.X_train_, self.X_test_, self.X_val_]):\n",
    "                joblib.dump(data, '../pkl/'+ name + sufix + '.pkl') \n",
    "            \n",
    "\n",
    "        print('Pickle Done')\n",
    "        \n",
    "    \n",
    "    def read_data(self, sufix):\n",
    "        self.X_train_ = joblib.load('../pkl/'+ 'X_train_' + sufix + '.pkl') \n",
    "        self.X_test_ = joblib.load('../pkl/'+ 'X_test_' + sufix + '.pkl') \n",
    "        self.X_val_ = joblib.load('../pkl/'+ 'X_val_' + sufix + '.pkl') \n",
    "        print('Done Loading Data: ', 'X_train_' + sufix, '  X_test_' + sufix, '  X_val_' + sufix )   \n",
    "    \n",
    "   \n",
    "    def get_members_evaluations_all(self):\n",
    "        return Analytica_2.evaluations_all.keys() # pega somente os modelos que acionaram evaluate_HP\n",
    "    \n",
    "    \n",
    "    def extract_model_instance(self, my_name):\n",
    "        ''' my_name must be a single value string. \n",
    "            Returns the model instance out of the pipeline. ( pipelilne stored in evaluaions all) \n",
    "            Intended to export the fitted model and it`s best params for reuse (bagging).\n",
    "        '''\n",
    "#         my_name_no_under = my_name.split('_')[0]\n",
    "#         return Super_Analytica.evaluations_all[my_name]['best_estimator'].named_steps[my_name_no_under]\n",
    "        return Analytica_2.evaluations_all[my_name]['best_estimator']\n",
    "\n",
    "\n",
    "#######################.         PLOT            ####################       \n",
    "#######################.         PLOT            ####################   \n",
    "      \n",
    "     \n",
    "#     def Evaluate_HP_val(self, classifier, space, my_name, pkl_W=False):     \n",
    "    def plot_learning_curve_2(self, load_saved=True, chosen=None):  \n",
    "        '''\n",
    "            If load_saved == False it will overwrite any file named equal\n",
    "            chosen must be a list of members of evaluations_all\n",
    "            if chosen == None, it will process all the models in evaluations all\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        \n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members)) # all chosen in evaluations_all\n",
    "            list_members = chosen # check if chosen are in evaluations all\n",
    "            list_members.sort()     \n",
    "        \n",
    "        for model in list_members :\n",
    "            path = '../pkl/l_curve_'+ model + '.png'\n",
    "            if load_saved == True:\n",
    "#                 print(type(name))\n",
    "                print('opening the image',  model)\n",
    "        \n",
    "                display(Image(path))\n",
    "            else:\n",
    "                \n",
    "                mdl = Analytica_2.evaluations_all[model]['best_estimator']\n",
    "#                 print(mdl)\n",
    "                sizes = np.linspace(0.2, 1.0, 5)\n",
    "                viz = LearningCurve(mdl, train_sizes=sizes, scoring='roc_auc')\n",
    "                viz.fit(self.X_train_, y_train)\n",
    "                viz.poof(outpath=path)\n",
    "                print('saving l_curve_ image for ',  model)\n",
    "                plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_validation_curve_2(self, chosen=None):\n",
    "        '''\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "        \n",
    "        for name in list_members:\n",
    "            self.helper_plot_validation_curve(name)\n",
    "        \n",
    "    \n",
    "    def helper_plot_validation_curve(self, my_name):\n",
    "        \n",
    "        data4 = Analytica_2.helper_translate(self, my_name)\n",
    "        for param2, vals2 in data4.iteritems(): #iterar sobre o data4 que é o geral que contem todas as colunas\n",
    "#             print('param2: ',  param2)\n",
    "            if param2 == 'losses':\n",
    "                continue\n",
    "\n",
    "            ### HYPEROPT VALIDATION CURVE - LOSS FUNCTION\n",
    "            data_temp = None \n",
    "            data_temp = data4[[ param2, 'losses']].dropna().copy()\n",
    "#             print('param2__: ',  param2)\n",
    "            x = data_temp[param2]\n",
    "            y = data_temp.losses\n",
    "            plt.scatter(x,y, alpha=0.2, c=y, cmap='RdBu')\n",
    "            plt.xlabel(param2)\n",
    "            plt.ylabel('Score')\n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "    def helper_translate(self, my_name):   #plot_validation_curve\n",
    "        # translates FROM STEPS(len) TO the chosen range of VALUES\n",
    "        data4 = Analytica_2.helper_translate_2(self, my_name)\n",
    "        P = Analytica_2.evaluations_all[my_name]['param_grid']\n",
    "        hyperopt_list = [hyperopt.pyll_utils.str(v) for k,v in P.items()] # abrindo o hyperopt.utils numa lista of long strings. one string for each param\n",
    "    #     print(hyperopt_list)\n",
    "\n",
    "        def helper_split_string(s): # breaks up the long string and remove ''. returns a list of strings\n",
    "            ls = [i for i in s.split(' ')] \n",
    "            for i in range(len(ls)):\n",
    "                if '' in ls:\n",
    "                    ls.remove('')\n",
    "            return ls\n",
    "\n",
    "        hyperopt_list2 = list(map(helper_split_string, hyperopt_list))   # lista hyperopt.utils ready to be analysed for patterns\n",
    "#         print(hyperopt_list2)\n",
    "\n",
    "        pat = r'\\w+\\{(\\w+)\\}\\W+[n3]'\n",
    "#         pat2 = r'\\w+\\{(\\d+)\\}'\n",
    "        pat2 = r'\\{([0-9]*[.]?[0-9]+)\\}'\n",
    "\n",
    "\n",
    "        D = {}     # dict to store the whole range of real values\n",
    "        for L in hyperopt_list2:  # procurar pelo param que comeca com X e retrieve values\n",
    "            if len(L) == 1:\n",
    "                break\n",
    "            else:\n",
    "                w = L[3]  # captura o 4 termo\n",
    "                if re.search(pat, w): # checa se dentro dos {} tem uma palavra\n",
    "            #         print('yes')\n",
    "                    m = re.search(pat, w).groups()[0] # var com o termo capturado, que é um key de um dos dicts  de param_1\n",
    "                    if m.startswith('zz_'):  # Checa se o termo capturado inicia com 'zz_'. CAso sim, processa adiante.\n",
    "#                         print(m)\n",
    "#                         print(L[6:])\n",
    "                        D[m] = [re.search(pat2, i).groups()[0] for i in L[6:]] # varre todos os mebros apos o 6 membro capturando o integer via o pat2. coleta todo o range de valores para cada param. Cada key é um param com prefixo 'X'.\n",
    "            #             print(D[m])\n",
    "                        data4[m] = data4[m].map(lambda x: D[m][int(x)]).astype('float64') # map each value x of the given column data4[m] with the index x in the respect dict D[m]. x represent the step     update the column in data4. Columns have 'X' prefix\n",
    "        data4.columns = data4.columns.map(lambda x: x.strip('X') if x.startswith('X') else x) # renomeia as colunas para nao ter X\n",
    "        data4 = data4.reindex(sorted(data4.columns), axis=1) # order the columns afabetically\n",
    "#         print('end translate')\n",
    "        ### END DATA PREP\n",
    "        return data4\n",
    "    \n",
    "    \n",
    "    def helper_translate_2(self, my_name):   # plot_validation_curve\n",
    "        '''ingest the Trials dict, clean it and return a Data Frame with the sampled values to be updated ahead.'''\n",
    "\n",
    "        data3 = Analytica_2.evaluations_all[my_name]['T']\n",
    "        d = [i['misc']['vals'] for i in  data3.trials]\n",
    "\n",
    "        L = []\n",
    "        for i in  d:\n",
    "            d_transformed = {k:v[0] if v !=[] else v  for k,v in i.items()} # retirando os valores de dentro das lista.\n",
    "            L.append(d_transformed)\n",
    "    #     L[0]\n",
    "\n",
    "        param_list = [list(i['misc']['vals'].keys())  for i in data3.trials[:1]][0]\n",
    "        param_list_noChoices =  [i for i in param_list if i != 'choices' ]\n",
    "\n",
    "        data4 = pd.DataFrame(L)\n",
    "        data4['losses'] = [1-i for i in data3.losses()] # invertendo de losses --> score\n",
    "        data4 = data4.applymap(lambda x: np.nan if x == [] else x) # transforma os [] em NaN - para qnd houver choice boladao\n",
    "        return data4 # este dataset contem as colunas'X' que deverao ser traduzidas posteriormente\n",
    "    \n",
    "    \n",
    "#######################.         EVALUATE            ####################       \n",
    "#######################.         EVALUATE            ####################                    \n",
    "\n",
    "        # new\n",
    "    def Evaluate_HP_val(self, classifier, space, my_name, pkl_W=False): \n",
    "        '''\n",
    "            classifier must be a string representing a clf_all member. space must be a string representing a key from space_all.\n",
    "            Essa funcao realiza parameter tunning via validation set e aplica best_model em todo dataset de treino.\n",
    "\n",
    "        '''\n",
    "        self.my_name = my_name\n",
    "        Analytica_2.insert_member(self)\n",
    "        start= timer()\n",
    "\n",
    "        self.instance = clone(clf_all[classifier])  #  to avoid edit the original object\n",
    "#         print('self.instance0: ', self.instance)\n",
    "           \n",
    "        def objective(params):\n",
    "#             print('self.instance1: ', self.instance, '\\n')\n",
    "            self.instance.set_params(**params)  \n",
    "#             print('self.instance2: ', self.instance, '\\n')\n",
    "            mdl = self.instance.fit(self.X_train_, y_train) # n_jobs=-1) \n",
    "            y_pred_val = mdl.predict(self.X_val_)\n",
    "            score = roc_auc_score(y_val, y_pred_val)\n",
    "            print('val score: ', score)\n",
    "            return 1 - score\n",
    "        \n",
    "            \n",
    "        T = Trials()\n",
    "        space_ = space_all[space] \n",
    "        best = fmin(objective, space_, algo=tpe.suggest, max_evals=2, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space_, best) # is the translation of best. Real Values, not steps.\n",
    "#         print('best_params :', best_params, '\\n')\n",
    "        best_model = self.instance.set_params(**best_params)\n",
    "        best_model.fit(self.X_train_, y_train) \n",
    "#         print('best_model :', best_model, '\\n')\n",
    "        \n",
    "        y_pred = best_model.predict_proba(self.X_test_)[:, 1]\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('acc :', acc, '\\n')\n",
    "        \n",
    "        pipe_best = clone(self.pipe)\n",
    "        pipe_best.steps.append([self.my_name, best_model]) # params so do modelo\n",
    "#         print('pipe_best: ', pipe_best)\n",
    "        \n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Analytica_2.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': best_model, \\\n",
    "                                 'param_grid': space_, 'y_pred': y_pred, \\\n",
    "                                 'pipe_best': pipe_best, 'instance':self.instance,\\\n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "                \n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Analytica_2.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP_val and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP_val done in ', minutes_elapsed, ' minutes') \n",
    "\n",
    "\n",
    "\n",
    "        # new\n",
    "    def Evaluate_HP_cv(self, classifier, space,  my_name, pkl_W=False): \n",
    "      \n",
    "        self.my_name = my_name\n",
    "        Analytica_2.insert_member(self)\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "\n",
    "        self.instance = clone(clf_all[classifier])  #  to avoid edit the original object\n",
    "#         print('self.instance: ', self.instance)\n",
    "\n",
    "        def objective(params):\n",
    "#             pipe_full.set_params(**params)  # diferente do caso sem pipe\n",
    "            self.instance.set_params(**params)  \n",
    "#             print('self.instance: ', self.instance)\n",
    "            score = cross_val_score(self.instance, self.X_train_, y_train, scoring='roc_auc', cv=kfold) # n_jobs=-1) \n",
    "#             print('cv score:', score.mean())\n",
    "            return 1 - score.mean()\n",
    "\n",
    "\n",
    "            \n",
    "        T = Trials()\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=40, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) # is the translation of best. Real Values, not steps.\n",
    "#         print('best_params :', best_params, '\\n')\n",
    "        best_model = self.instance.set_params(**best_params)\n",
    "        best_model.fit(self.X_train_, y_train) \n",
    "#         print('best_model :', best_model, '\\n')\n",
    "        \n",
    "        y_pred = best_model.predict_proba(self.X_test_)[:, 1]\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('acc :', acc, '\\n')\n",
    "        \n",
    "        pipe_best = clone(self.pipe)\n",
    "#         print('self.pipe: ',  self.pipe)\n",
    "        pipe_best.steps.append([self.my_name, best_model]) # params so do modelo\n",
    "#         print('pipe_best: ', pipe_best)\n",
    "        \n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Analytica_2.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': best_model, \\\n",
    "                                 'param_grid': None, 'y_pred': y_pred, \\\n",
    "                                 'pipe_best': pipe_best, 'instance':self.instance,\\\n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "                \n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Analytica_2.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP_cv and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP_cv done in ', minutes_elapsed, ' minutes') \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_models(s):\n",
    "    '''look in md_all for the models that starts with s return it in a list os strings '''\n",
    "    all_names = [name for L in md_all for name in L] # from list of lists to a single list\n",
    "    return [name for name in all_names if name.startswith(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "333px",
    "left": "1376px",
    "top": "505px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
