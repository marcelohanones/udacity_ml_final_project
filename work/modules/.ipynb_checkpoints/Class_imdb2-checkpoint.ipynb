{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Super_Analytica(object):\n",
    "    WHO_LIST = []    #. classe Super nao fica registrada na WHOLIST.\n",
    "    evaluations_all = {}\n",
    "    \n",
    "    def __init__(self, my_name):\n",
    "        self.my_name = my_name\n",
    "#         self.WHO_LIST = []\n",
    "        \n",
    "    def insert_member(self):   # only used in the Sub init\n",
    "        Super_Analytica.WHO_LIST.append(self.my_name)  ## APAGAR DEPOIS\n",
    "#         if not self.my_name in Super_Analytica.WHO_LIST:\n",
    "#             Super_Analytica.WHO_LIST.append(self.my_name)\n",
    "#             print('\\n')\n",
    "#             print(self.my_name, '  Instance Created')\n",
    "#         else:\n",
    "#             raise ValueError(self.my_name, ' This instance is already in WHOLIST. Or pick another name.')\n",
    "            \n",
    "    def plot_ranking(self):\n",
    "#         my_name_list = Super_Analytica.WHO_LIST\n",
    "#         scores = [Super_Analytica.evaluations_all[name]['score'] for name in my_name_list] \n",
    "#         g = sns.barplot(scores, my_name_list, orient='h', palette=\"Blues\" )\n",
    "#         for index, row in enumerate( scores):\n",
    "#             g.text(row, index, round(row*100,2), color='black', ha=\"right\") \n",
    "        data = pd.DataFrame(Super_Analytica.evaluations_all).T\n",
    "#         print(data)\n",
    "        data.sort_values('score', inplace=True)\n",
    "        data.reset_index(inplace=True) # in order to display the values in text at the top of each barplot, the index need to be numeric.\n",
    "        data.rename(columns = {'index': 'my_name'}, inplace=True)\n",
    "        g = sns.barplot(data.score, data.my_name, orient='h', palette=\"Blues\" )\n",
    "        for index, row in data['score'].iteritems():\n",
    "            g.text(row, index, round(row*100,2), color='black', ha=\"right\") \n",
    "            \n",
    "    def export_best_models(self, n):\n",
    "        data = pd.DataFrame(Super_Analytica.evaluations_all).T\n",
    "        data.score = data.score.astype(float)\n",
    "        s = data.nlargest(n, ['score'])['best_estimator']\n",
    "        return list(s.iteritems())\n",
    "\n",
    "\n",
    "    def pkl_W_evaluations_all(self, filename):    # save evaluation all to disk\n",
    "        '''filename is a string like KNN_1'''\n",
    "        pickle_w = open('../pkl/'+ filename + '.pkl', 'wb')\n",
    "        obj = Super_Analytica.evaluations_all\n",
    "        pickle.dump(obj, pickle_w, pickle.HIGHEST_PROTOCOL)\n",
    "        pickle_w.close()\n",
    "        print('Done. Pickled to disk')\n",
    "        \n",
    "    def pkl_R(self, filename):     # import a pickle into the 'cloud'\n",
    "        ''' filename must be a string like KNN_1\n",
    "        import a pickle into the 'cloud'\n",
    "        '''\n",
    "#         pickle_r = open('../pkl/'+ filename + '.pkl', 'rb')\n",
    "#         back_from_dead = pickle.load(pickle_r)\n",
    "        back_from_dead = joblib.load('../pkl/'+ filename + '.pkl') \n",
    "        #check duplicates and insert into evaluations_all\n",
    "        keys_to_import = list(back_from_dead.keys())\n",
    "        keys_members = list(Super_Analytica.evaluations_all.keys())\n",
    "        Super_Analytica.evaluations_all.update(back_from_dead)\n",
    "        print('Done. Evaluation pickled from pisk')\n",
    "        \n",
    "        if any(i in keys_to_import for i in keys_members):\n",
    "            print (' One or more evaluations were overwriten')\n",
    "\n",
    " \n",
    "         \n",
    "        \n",
    "    def get_members_evaluations_all(self):\n",
    "        return Super_Analytica.evaluations_all.keys() # pega somente os modelos que acionaram evaluate_HP\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            ##############     PLOTTING SECTION     ###############\n",
    "    \n",
    "    def plot_learning_curve(self, load_saved=True, chosen=None):  #loop over\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            filtered = filter(lambda x: x in list_members, chosen) # check chosen are in evaluations all\n",
    "        \n",
    "        for name in filtered :\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                image = mpimg.imread('../pkl/l_curve_'+ name + '.png')\n",
    "#                 Image(filename='../pkl/l_curve_'+ name + '.png')\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "                clf = Super_Analytica.evaluations_all[name]['best_estimator']\n",
    "                self.helper_plot_learning_curve(clf, name)\n",
    "            \n",
    "\n",
    "    def helper_plot_learning_curve(self, clf, name):    \n",
    "        sizes = np.linspace(0.1, 1.0, 10)\n",
    "        viz = LearningCurve(clf, train_sizes=sizes, scoring='accuracy', n_jobs=-1)\n",
    "        viz.fit(X_train, y_train)\n",
    "#         viz.poof()\n",
    "        viz.poof(outpath='../pkl/l_curve_'+ name + '.png')\n",
    "        print('saving the image',  name)\n",
    "        plt.clf()\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    def plot_validation_curve(self, chosen=None):\n",
    "        '''\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            filtered = filter(lambda x: x in list_members, chosen) # check if chosen are in evaluations all\n",
    "        \n",
    "        for name in filtered:\n",
    "            self.helper_plot_validation_curve(name)\n",
    "        \n",
    "    \n",
    "    def helper_plot_validation_curve(self, my_name):\n",
    "        \n",
    "        data4 = Super_Analytica.translate(self, my_name)\n",
    "#         return data4\n",
    "        # ATENTION: hp.choice with 5 items or less will be taken as categorical and removed. \n",
    "#         col_rid = [name for name, vals in data4.iteritems() if set(vals.unique()).issubset(set([0,1,2]))]    # avoid columns of no numerical hp.choice. ones that are made of strings as alternatives.\n",
    "        col_rid = [] \n",
    "        col_rid.append('losses')\n",
    "        print(col_rid)\n",
    "        print(data4.columns)\n",
    "        # iterar sobre data4 e data5 para plotar\n",
    "        # clf_2 = data.best_estimator.get_values()[0]\n",
    "        clf_2 = Super_Analytica.evaluations_all[my_name]['best_estimator']\n",
    "        for param2, vals2 in data4.iteritems(): #iterar sobre o data4 que é o geral que contem todas as colunas\n",
    "#             print('param2: ',  param2)\n",
    "            if param2 == 'losses':\n",
    "                continue\n",
    "\n",
    "#             ### YELLOWBRICK VALIDATION CURVE    \n",
    "#             if param2 not in col_rid:  # se for um parametro do data5, prosseguir\n",
    "# #                 print('param2_: ',  param2)\n",
    "#                 param_range = np.sort(list(set(np.random.choice(vals2, 20)))) # sampleia 20 pontos do total. Sem repeticao\n",
    "#                 viz = ValidationCurve(clf_2, param_name=param2, param_range=param_range, scoring='accuracy', n_jobs=-1)\n",
    "#                 viz.fit(X_train, y_train)\n",
    "#                 viz.poof()\n",
    "#                 plt.show()\n",
    "\n",
    "            ### HYPEROPT VALIDATION CURVE - LOSS FUNCTION\n",
    "            data_temp = None # used exclusively on HERE \n",
    "            data_temp = data4[[ param2, 'losses']].dropna().copy()\n",
    "            print('param2__: ',  param2)\n",
    "            x = data_temp[param2]\n",
    "            y = data_temp.losses\n",
    "            plt.scatter(x,y, alpha=0.2, c=y, cmap='RdBu')\n",
    "            plt.xlabel(param2)\n",
    "            plt.ylabel('Score')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "   \n",
    "    def plot_models_corr(self, chosen=None):\n",
    "        if chosen:\n",
    "            corr = pd.DataFrame({k:v['y_pred'] for k,v in self.evaluations_all.items() if k in chosen}).corr()\n",
    "        else:\n",
    "            corr = pd.DataFrame({k:v['y_pred'] for k,v in self.evaluations_all.items()}).corr()\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(14,10)); \n",
    "        sns.heatmap(corr, annot=True, cmap=\"YlGnBu\", ax=ax)    \n",
    "\n",
    "        \n",
    "        \n",
    "    def plot_Classification_Report(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            filtered = filter(lambda x: x in list_members, chosen)\n",
    "\n",
    "        for name in filtered:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                image = mpimg.imread('../pkl/c_reprt_'+ name + '.png')\n",
    "                plt.clf()\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "            else:\n",
    "\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = ClassificationReport(clf, support=True)\n",
    "\n",
    "                visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "                visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "                visualizer.poof(outpath='../pkl/c_reprt_'+ name + '.png')  # Draw/show/poof the data\n",
    "                plt.clf()\n",
    "                print('saving c_reprt_ image for',  name)       \n",
    "            \n",
    "\n",
    "    def plot_Confusion_Matrix(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            filtered = filter(lambda x: x in list_members, chosen)\n",
    "\n",
    "        for name in filtered:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                image = mpimg.imread('../pkl/c_matrx_'+ name + '.png')\n",
    "                plt.clf()\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = ConfusionMatrix(clf, percent=True)\n",
    "                visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "                visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "                visualizer.poof(outpath='../pkl/c_matrx_'+ name + '.png')  # Draw/show/poof the data\n",
    "                plt.clf()\n",
    "                print('saving c_matrx_ image for',  name)       \n",
    "            \n",
    "\n",
    "    def plot_ROC_AUC(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            filtered = filter(lambda x: x in list_members, chosen)\n",
    "\n",
    "        for name in filtered:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                image = mpimg.imread('../pkl/roc_auc_'+ name + '.png')\n",
    "                plt.clf()\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = ROCAUC(clf)\n",
    "                visualizer.fit(X_train, y_train)  \n",
    "                visualizer.score(X_test, y_test)  \n",
    "                visualizer.poof(outpath = '../pkl/roc_auc_'+ name + '.png')  \n",
    "                plt.clf()\n",
    "                print('saving roc_auc_ image for',  name)       \n",
    "     \n",
    "    \n",
    "    # isso eh igual ao histograma do y ?\n",
    "    def plot_Class_Balance(self, chosen=None):\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            list_members = filter(lambda x: x in chosen, list_members)\n",
    "\n",
    "        for name in list_members:\n",
    "            clf = self.evaluations_all[name]['best_estimator']\n",
    "            visualizer = ClassBalance(clf)\n",
    "\n",
    "            visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "            visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "            visualizer.poof()             # Draw/show/poof the data\n",
    "\n",
    "\n",
    "    # NOT WORKING\n",
    "    def plot_Class_Prediction_Error(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            filtered = filter(lambda x: x in list_members, chosen)\n",
    "\n",
    "        for name in filtered:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                image = mpimg.imread('../pkl/c_pred_err_'+ name + '.png')\n",
    "                plt.clf()\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = ClassPredictionError(clf)\n",
    "                visualizer.fit(X_train, y_train)  \n",
    "                visualizer.score(X_test, y_test)  \n",
    "                visualizer.poof(outpath = '../pkl/c_pred_err_'+ name + '.png')  \n",
    "                plt.clf()\n",
    "                print('saving c_pred_err_ image for',  name)       \n",
    "     \n",
    "\n",
    "    def plot_Discrimination_Threshold(self,load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            filtered = filter(lambda x: x in list_members, chosen)\n",
    "\n",
    "        for name in filtered:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                image = mpimg.imread('../pkl/disc_thres_'+ name + '.png')\n",
    "                plt.clf()\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = DiscriminationThreshold(clf)\n",
    "                visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "                visualizer.poof(outpath = '../pkl/disc_thres_'+ name + '.png')  \n",
    "                plt.clf()\n",
    "                print('saving disc_thres_ image for',  name)       \n",
    "    \n",
    "    # NOT WORKING\n",
    "    # tentar juntar com PCA ja que o plot é 2D \n",
    "    def plot_Decision_Boundaries(self,load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            filtered = filter(lambda x: x in list_members, chosen)\n",
    "\n",
    "        for name in filtered:\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                image = mpimg.imread('../pkl/dec_bound_'+ name + '.png')\n",
    "                plt.clf()\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "            else:\n",
    "                clf = self.evaluations_all[name]['best_estimator']\n",
    "                visualizer = DecisionBoundariesVisualizer(clf)\n",
    "                visualizer.fit(X_train, y_train)  \n",
    "                visualizer.poof(outpath = '../pkl/dec_bound_'+ name + '.png')  \n",
    "                plt.clf()\n",
    "                print('saving dec_bound_ image for',  name)       \n",
    "     \n",
    "\n",
    "    def plot_Viz_pipe(self, pipe, name, load_saved=True):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "            'pca_proj_', 'manif_' \n",
    "            plot PCA PROJECTION or Manifold as last pipeline step\n",
    "        '''\n",
    "        if load_saved == True:\n",
    "            print('opening the image',  name)\n",
    "            image = mpimg.imread('../pkl/'+ name + '.png')\n",
    "            plt.clf()\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "        else:\n",
    "            pipe.fit_transform(X_train, y_train.values.ravel())\n",
    "            pipe.named_steps['viz'].poof(outpath='../pkl/'+ name + '.png')\n",
    "            print('saving viz image for',  name)       \n",
    "\n",
    "\n",
    "\n",
    "    def plot_Feature_Importances(self, X_train, y_train, chosen=None, col_filter=None):\n",
    "        '''sklearn`s '''\n",
    "        if col_filter:    # to filter by  \n",
    "            X_train = X_train[col_filter]\n",
    "            \n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            list_members = filter(lambda x: x in chosen, list_members)\n",
    "\n",
    "        for name in list_members:\n",
    "            clf = self.evaluations_all[name]['instance']\n",
    "            visualizer = FeatureImportances(clf)\n",
    "\n",
    "            visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "            visualizer.poof()             # Draw/show/poof the data\n",
    "\n",
    "            \n",
    "    # falta ordenar e anotar os valores          \n",
    "    def plot_feature_importance_mapper(self, pipe, mapper, est):\n",
    "        '''est is a string of the primitive estimator`s name referenced in the Pipeline steps.\n",
    "        ex: 'DT' ,'KNN'  '''\n",
    "        y = mapper.transformed_names_\n",
    "        x = pipe.named_steps[est].feature_importances_\n",
    "        sns.barplot(x, y)\n",
    "        \n",
    "                \n",
    "    def explain_weights(self, name):\n",
    "        '''ELI 5'''\n",
    "        est = Super_Analytica.evaluations_all[name]['instance']\n",
    "        vec = map_tfidf.features[0][1][1]\n",
    "        return eli5.show_weights(est, vec=vec)\n",
    "\n",
    "    \n",
    "    \n",
    "    def explain_prediction(self, name):\n",
    "        '''ELI 5'''\n",
    "        est = Super_Analytica.evaluations_all[name]['instance']\n",
    "        vec = map_tfidf.features[0][1][1]\n",
    "        return eli5.show_prediction(est, X_train.iloc[1,:][0] , vec=vec)\n",
    "     \n",
    "        \n",
    "    def get_feature_names(self, my_name, pipe_steps):\n",
    "        ''' tekes two strings and return the feature names '''\n",
    "        p = Super_Analytica.evaluations_all[my_name]['best_estimator'] # acessa cloud e pega o best estimator\n",
    "    \n",
    "        return p.get_params()[pipe_steps].get_feature_names()\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "##############     EVALUATION SECTION     ###############\n",
    "    \n",
    "    # pipeline not implemented\n",
    "    def evaluate_CV(self, models, X_train, y_train, chosen=None, col_filter=None):\n",
    "        ''' iterates over a set of models and not by Evaluations_all\n",
    "            \n",
    "        '''\n",
    "#         if col_filter:    # to filter by  \n",
    "#             X_train = X_train[col_filter]\n",
    "        \n",
    "        models_to_process = models.keys()\n",
    "        if chosen:\n",
    "            models_to_process = filter(lambda x: x in chosen, models_to_process)\n",
    "        for name in models_to_process:\n",
    "            clf = models[name]\n",
    "            y_pred = cross_val_predict(clf, X_train, y_train, cv=10, n_jobs=-1)\n",
    "            acc = cross_val_score(clf, X_train, y_train, cv=10, n_jobs=-1).mean() \n",
    "            print('acc :', acc)\n",
    "            Super_Analytica.evaluations_all[name] = {'score': acc, 'best_estimator': None, \\\n",
    "                                 'param_grid': None, 'y_pred': y_pred, \\\n",
    "                                 'param_pipe': None, 'instance':clf,\\\n",
    "                                 'best_params' : None, 'T': None} \n",
    "        \n",
    "            print(name, ' Cross Validation is Done' )\n",
    "   \n",
    "\n",
    "\n",
    "    def translate(self, my_name):   #plot_validation_curve\n",
    "            # in: data4 , param_grid\n",
    "        # out: translated data4 \n",
    "        # varre todo o param_grid a procura dos param X e faz a traducao em data4\n",
    "    #     super1.evaluations_all['DT_7']['param_grid']\n",
    "\n",
    "        # translates FROM STEPS(len) TO the chosen range of VALUES\n",
    "        data4 = Super_Analytica.make_data4(self, my_name)\n",
    "        P = Super_Analytica.evaluations_all[my_name]['param_grid']\n",
    "        hyperopt_list = [hyperopt.pyll_utils.str(v) for k,v in P.items()] # abrindo o hyperopt.utils numa lista of long strings. one string for each param\n",
    "    #     print(hyperopt_list)\n",
    "\n",
    "        def helper_split_string(s): # breaks up the long string and remove ''. returns a list of strings\n",
    "            ls = [i for i in s.split(' ')] \n",
    "            for i in range(len(ls)):\n",
    "                if '' in ls:\n",
    "                    ls.remove('')\n",
    "            return ls\n",
    "\n",
    "        hyperopt_list2 = list(map(helper_split_string, hyperopt_list))   # lista hyperopt.utils ready to be analysed for patterns\n",
    "#         print(hyperopt_list2)\n",
    "\n",
    "        pat = r'\\w+\\{(\\w+)\\}\\W+[n3]'\n",
    "#         pat2 = r'\\w+\\{(\\d+)\\}'\n",
    "        pat2 = r'\\{([0-9]*[.]?[0-9]+)\\}'\n",
    "\n",
    "\n",
    "        D = {}     # dict to store the whole range of real values\n",
    "        for L in hyperopt_list2:  # procurar pelo param que comeca com X e retrieve values\n",
    "            if len(L) == 1:\n",
    "                break\n",
    "            else:\n",
    "                w = L[3]  # captura o 4 termo\n",
    "                if re.search(pat, w): # checa se dentro dos {} tem uma palavra\n",
    "            #         print('yes')\n",
    "                    m = re.search(pat, w).groups()[0] # var com o termo capturado, que é um key de um dos dicts  de param_1\n",
    "                    if m.startswith('zz_'):  # Checa se o termo capturado inicia com 'zz_'. CAso sim, processa adiante.\n",
    "#                         print(m)\n",
    "#                         print(L[6:])\n",
    "                        D[m] = [re.search(pat2, i).groups()[0] for i in L[6:]] # varre todos os mebros apos o 6 membro capturando o integer via o pat2. coleta todo o range de valores para cada param. Cada key é um param com prefixo 'X'.\n",
    "            #             print(D[m])\n",
    "                        data4[m] = data4[m].map(lambda x: D[m][int(x)]).astype('float64') # map each value x of the given column data4[m] with the index x in the respect dict D[m]. x represent the step     update the column in data4. Columns have 'X' prefix\n",
    "        data4.columns = data4.columns.map(lambda x: x.strip('X') if x.startswith('X') else x) # renomeia as colunas para nao ter X\n",
    "        data4 = data4.reindex(sorted(data4.columns), axis=1) # order the columns afabetically\n",
    "#         print('end translate')\n",
    "        ### END DATA PREP\n",
    "        return data4\n",
    "    \n",
    "    \n",
    "    def make_data4(self, my_name):   # plot_validation_curve\n",
    "        '''ingest the Trials dict, clean it and return a Data Frame with the sampled values to be updated ahead.'''\n",
    "\n",
    "    #     data3 = super1.evaluations_all['DT_7']['T']\n",
    "        data3 = Super_Analytica.evaluations_all[my_name]['T']\n",
    "    #     data3\n",
    "\n",
    "        d = [i['misc']['vals'] for i in  data3.trials]\n",
    "    #     d[:3]\n",
    "\n",
    "        L = []\n",
    "        for i in  d:\n",
    "            d_transformed = {k:v[0] if v !=[] else v  for k,v in i.items()} # retirando os valores de dentro das lista.\n",
    "            L.append(d_transformed)\n",
    "    #     L[0]\n",
    "\n",
    "        param_list = [list(i['misc']['vals'].keys())  for i in data3.trials[:1]][0]\n",
    "        param_list_noChoices =  [i for i in param_list if i != 'choices' ]\n",
    "    #     param_list\n",
    "    #     param_list_noChoices\n",
    "\n",
    "        data4 = pd.DataFrame(L)\n",
    "        data4['losses'] = [1-i for i in data3.losses()] # invertendo de losses --> score\n",
    "        data4 = data4.applymap(lambda x: np.nan if x == [] else x) # transforma os [] em NaN - para qnd houver choice boladao\n",
    "        return data4 # este dataset contem as colunas'X' que deverao ser traduzidas posteriormente\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T03:54:32.442429Z",
     "start_time": "2018-05-19T03:54:32.411577Z"
    },
    "code_folding": [
     13,
     61
    ]
   },
   "outputs": [],
   "source": [
    "class Analytica_Bolada(object):\n",
    "    \n",
    "#     def __init__(self, models, my_name):\n",
    "    def __init__(self, instance, param_grid, my_name):\n",
    "                \n",
    "#         Super_Analytica.__init__(self,  my_name)\n",
    "#         super().__init__(  my_name)\n",
    "        self.my_name = my_name\n",
    "        self.instance = instance\n",
    "        self.param_grid = param_grid\n",
    "#         self.models = models\n",
    "#         self.models = dict(self.my_name=(instance, param_grid))\n",
    "#         print(self.models)\n",
    "        Super_Analytica.insert_member(self)\n",
    "        \n",
    "        \n",
    "\n",
    "    def plot_learning_curve(self):\n",
    "        clf = Super_Analytica.evaluations_all[self.my_name]['best_estimator']\n",
    "#         print(clf)\n",
    "        Super_Analytica.helper_plot_learning_curve(self, clf, self.my_name)\n",
    "        \n",
    "\n",
    "        \n",
    "    def plot_validation_curve(self):\n",
    "        Super_Analytica.helper_plot_validation_curve(self, self.my_name)\n",
    "   \n",
    "    \n",
    "#     def evaluate_HP(self, param_pipe=None, feature_space=None, pkl_W=False):  # sub\n",
    "#         '''\n",
    "#         internamente sempre vai ter um pipeline mesmo que na declaracao da funcao nao tenha pipeline\n",
    "#         '''\n",
    "#         result_dict = {}\n",
    "#         X_train_internal = X_train\n",
    "#         X_test_internal = X_test\n",
    "        \n",
    "#         if feature_space:    # to filter by RFE columns \n",
    "#             X_train_internal = X_train[feature_space].copy() \n",
    "#             X_test_internal = X_test[feature_space].copy() \n",
    "        \n",
    "# #         for key,value in self.models.items():  # # iterate over clf\n",
    "# #             instance, param_grid, my_name\n",
    "#         key = self.my_name.split('_')[0] # to use in pipe. From KNN_2 -> KNN. Must mach param_grid prefix\n",
    "# #         print('key: ',  key)\n",
    "# #             instance = value[0]\n",
    "#         instance = self.instance\n",
    "# #             param_grid = value[1]\n",
    "#         param_grid = self.param_grid\n",
    "\n",
    "# #             print('instance before fit \\n', instance)\n",
    "# #             print(param_grid)\n",
    "\n",
    "#         if param_pipe == None:               # sem pipeline at instantiation\n",
    "#             param_pipe = [(key,instance)]\n",
    "#             pipe = Pipeline(param_pipe) \n",
    "\n",
    "#         else:                             # com pipeline\n",
    "\n",
    "#             param_pipe.append((key,instance))  # append the classifier at the last position after the transformers\n",
    "#             pipe = Pipeline(param_pipe) \n",
    "\n",
    "\n",
    "#         def objective(params):\n",
    "#             pipe.set_params(**params)  # diferente do caso sem pipe\n",
    "#             score = cross_val_score(pipe, X_train_internal, y_train, scoring='accuracy', cv=kfold, n_jobs=-1)\n",
    "#             return 1 - score.mean()\n",
    "\n",
    "#         T = Trials()\n",
    "#         space = param_grid\n",
    "#         best = fmin(objective, space, algo=tpe.suggest, max_evals=5, trials=T)  #  <---------------------------HERE<-----\n",
    "\n",
    "#         best_params = space_eval(space, best) # is the translation of best. Real Values, not steps.\n",
    "#         pipe.set_params(**best_params)\n",
    "#         clf = pipe.fit(X_train_internal, y_train) \n",
    "\n",
    "#         y_pred = clf.predict(X_test_internal)  # predict over unseen data\n",
    "#         # nao seria mais barato usar accuracy score para nao ter que rodar predict de novo \n",
    "#         acc = pipe.score(X_test_internal, y_test) # DUVIDA poderia usar ROC score por exemplo ?\n",
    "\n",
    "#         Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': clf, \\\n",
    "#                                  'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "#                                  'param_pipe': param_pipe, 'instance':instance,\\\n",
    "#                                  'best_params' : best_params, 'T': T } \n",
    "        \n",
    "#         print(self.my_name, '  Done. Model is Fitted and Scored. Results in Evaluations_all.' )\n",
    "        \n",
    "# #         print('instance \\n', instance)\n",
    "#         print('best_params \\n', best_params)\n",
    "#         print('clf \\n', clf)\n",
    "        \n",
    "    \n",
    "# Parece que nao é necessario faer o pickle do modelo fitado de dentro da Ebaluate_HP \n",
    "# pois o Evaluation_all ja guarda o fitted model.\n",
    "\n",
    "#         if  pkl_W:\n",
    "#             self.pkl_W_fitted_model(clf) \n",
    "# #             from sklearn.externals import joblib\n",
    "# #             joblib.dump(clf, 'work/test_joblib.pkl') \n",
    "            \n",
    "            \n",
    "#             print(self.my_name, '  pickled to disk' )\n",
    " \n",
    "    \n",
    "    # assume pipe declarado stardard, e nao como no HP \n",
    "    def evaluate_CV(self, pipe):\n",
    "        \n",
    "        start= timer()\n",
    "        y_pred = cross_val_predict(pipe, X_train, y_train, cv=kfold, n_jobs=-1)\n",
    "        acc = cross_val_score(pipe, X_train, y_train, cv=kfold, n_jobs=-1).mean() \n",
    "        end = timer()\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': pipe, \n",
    "                                                         'param_grid': None, 'y_pred': y_pred, \n",
    "                                                         'param_pipe': pipe, 'instance':self.instance,\n",
    "                                                         'best_params' : None, 'T': None, 'time': end-start} \n",
    "\n",
    "        print(self.my_name, '  Cross Validation is Done') \n",
    "        \n",
    "    \n",
    "    \n",
    "    # assume pipe declarado stardard, e nao como no HP \n",
    "    def evaluate_GS(self, param_pipe=None):\n",
    "        \n",
    "        if param_pipe == None:               # sem pipeline at instantiation\n",
    "#             pipe = Pipeline(param_pipe) \n",
    "            est = self.instance\n",
    "\n",
    "        else:                             # com pipeline\n",
    "\n",
    "#             param_pipe.append((key,instance))  # append the classifier at the last position after the transformers\n",
    "            est = param_pipe \n",
    "            print(est)\n",
    "\n",
    "\n",
    "        \n",
    "        clf = GridSearchCV(est, param_grid=self.param_grid, cv=kfold)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': clf.best_score_, 'best_estimator': clf.best_estimator_, \\\n",
    "                                                         'param_grid': self.param_grid, 'y_pred': y_pred, \\\n",
    "                                                         'param_pipe': None, 'instance': self.instance,\\\n",
    "                                                         'best_params': clf.best_params_, 'T': None } \n",
    "\n",
    "        print(self.my_name, '  GridSearch Evaluation is Done' )\n",
    "       \n",
    "    \n",
    "    \n",
    "    # pipe not implemented\n",
    "    def evaluate_R_GS(self, param_pipe=None):\n",
    "        clf = RandomizedSearchCV(self.instance, param_grid=self.param_grid, cv=kfold)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': clf.best_score_, 'best_estimator': clf.best_estimator_, \\\n",
    "                                                         'param_grid': self.param_grid, 'y_pred': y_pred, \\\n",
    "                                                         'param_pipe': None, 'instance': self.instance,\\\n",
    "                                                         'best_params': clf.best_params_, 'T': None } \n",
    "\n",
    "        print(self.my_name, '  Randomized GridSearch Evaluation is Done' )\n",
    "\n",
    "    \n",
    "    \n",
    "#     # WORKING OK\n",
    "#     def pkl_W_fitted_model(self, clf):    # export evaluation all to disk\n",
    "#         pickle_w = open('work/'+ self.my_name + '.pkl', 'wb')\n",
    "#         pickle.dump(clf, pickle_w, pickle.HIGHEST_PROTOCOL)\n",
    "#         pickle_w.close()\n",
    "        \n",
    "#     def evaluate_simple(self, param_pipe=None):\n",
    "#         print('ta na sub')\n",
    "        \n",
    "#         key = self.my_name.split('_')[0] # to use in pipe. From KNN_2 -> KNN. Must mach param_grid prefix\n",
    "#         instance = self.instance\n",
    "#         param_grid = self.param_grid\n",
    "\n",
    "#         if param_pipe == None:               # sem pipeline at instantiation\n",
    "#             param_pipe = [(key,instance)]\n",
    "#             pipe = Pipeline(param_pipe) \n",
    "\n",
    "#         else:                             # com pipeline\n",
    "#             param_pipe.append((key,instance))  # append the classifier at the last position after the transformers\n",
    "#             pipe = Pipeline(param_pipe) \n",
    "\n",
    "        \n",
    "#         clf = pipe.fit(X_train, y_train) \n",
    "\n",
    "#         y_pred = clf.predict(X_test)  # predict over unseen data\n",
    "#         acc = pipe.score(X_test, y_test) # DUVIDA poderia usar ROC score por exemplo ?\n",
    "\n",
    "#         Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': clf, \\\n",
    "#                                  'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "#                                  'param_pipe': param_pipe, 'instance':instance,\\\n",
    "#                                  'best_params' : None, 'T': None} \n",
    "        \n",
    "#         print(self.my_name, '  Evaluation simpple is Done' )\n",
    "        \n",
    "\n",
    " \n",
    "    def evaluate_HP2(self, param_pipe, feature_space=None, pkl_W=False):  # sub\n",
    "        '''\n",
    "        so funciona com Pipeline e sendo declarado conforme o padrao. Nao é que nem no HP\n",
    "        '''\n",
    "        start = timer()\n",
    "        result_dict = {}\n",
    "        X_train_internal = X_train\n",
    "        X_test_internal = X_test\n",
    "#         print(type(X_test))\n",
    "#         print(X_test.shape)\n",
    "        \n",
    "#         if feature_space:    # to filter by RFE columns \n",
    "#             X_train_internal = X_train[feature_space].copy() \n",
    "#             X_test_internal = X_test[feature_space].copy() \n",
    "        \n",
    "#         for key,value in self.models.items():  # # iterate over clf\n",
    "#             instance, param_grid, my_name\n",
    "        key = self.my_name.split('_')[0] # to use in pipe. From KNN_2 -> KNN. Must mach param_grid prefix\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "        pipe = param_pipe\n",
    "\n",
    "\n",
    "        def objective(params):\n",
    "            pipe.set_params(**params)  # diferente do caso sem pipe\n",
    "            score = cross_val_score(pipe, X_train_internal, y_train, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "            return 1 - score.mean()\n",
    "        \n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=100, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) # is the translation of best. Real Values, not steps.\n",
    "        pipe.set_params(**best_params)\n",
    "        clf = pipe.fit(X_train_internal, y_train) \n",
    "\n",
    "        y_pred = clf.predict(X_test_internal)  # predict over unseen data\n",
    "        # nao seria mais barato usar accuracy score para nao ter que rodar predict de novo \n",
    "        acc = pipe.score(X_test_internal, y_test) # DUVIDA poderia usar ROC score por exemplo ?\n",
    "        print('acc :', acc)\n",
    "        end = timer()\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': clf, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "                                 'param_pipe': param_pipe, 'instance':instance,\\\n",
    "                                 'best_params' : best_params, 'T': T, 'time': end-start } \n",
    "                \n",
    "        \n",
    "        D = {}\n",
    "        D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "#         obj = Super_Analytica.evaluations_all[self.my_name]\n",
    "        if  pkl_W:\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "#             pickle_w = open('../pkl/clf_'+ self.my_name + '.pkl', 'wb')\n",
    "#             pickle.dump(clf, pickle_w, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "            \n",
    "            print(self.my_name, '  pickled to disk' )\n",
    " \n",
    "        print(self.my_name, '  HP2 Done. Model is Fitted and Scored. Results in Evaluations_all.' )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineRFE(Pipeline):\n",
    "\n",
    "    def fit(self, X_train, y_train=None, **fit_params):\n",
    "        super(PipelineRFE, self).fit(X_train, y_train, **fit_params)\n",
    "        self.feature_importances_ = self.steps[-1][-1].feature_importances_\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best method to find if fitted\n",
    "# for model in L:\n",
    "#     try:\n",
    "#         model.predict(X_test)\n",
    "#         print(model, 'is FITTED \\n')\n",
    "#     except ValueError:\n",
    "#         print(model, 'NOT FITTED \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "333px",
    "left": "1376px",
    "top": "505px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
