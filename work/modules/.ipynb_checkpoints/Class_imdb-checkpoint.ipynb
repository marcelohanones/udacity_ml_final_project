{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Super_Analytica(object):\n",
    "    WHO_LIST = []    #. classe Super nao fica registrada na WHOLIST.\n",
    "    evaluations_all = {}\n",
    "    \n",
    "    def __init__(self):\n",
    "            pass\n",
    "\n",
    "    def insert_member(self):   # only used in the Sub init\n",
    "#         Super_Analytica.WHO_LIST.append(self.my_name)  ## APAGAR DEPOIS\n",
    "        if not self.my_name in Super_Analytica.WHO_LIST:\n",
    "            Super_Analytica.WHO_LIST.append(self.my_name)\n",
    "            print(self.my_name, '  Instance Created')\n",
    "            print('\\n')\n",
    "        else:\n",
    "            raise ValueError(self.my_name, ' This instance is already in WHOLIST. Or pick another name.')\n",
    "            \n",
    "    def plot_ranking(self, chosen=None, top_n=30, var = 'score'):\n",
    "        '''chosen must be a list. var must be a single value string'''\n",
    "#         var = 'score'\n",
    "        data = pd.DataFrame(Super_Analytica.evaluations_all).T\n",
    "        if chosen:\n",
    "            data = data.loc[chosen, :]\n",
    "        data.score = data.score.astype('float') \n",
    "        data = data.sort_values('score')[-top_n:]\n",
    "        data.reset_index(inplace=True) # in order to display the values in text at the top of each barplot, the index need to be numeric.\n",
    "        data.rename(columns = {'index': 'my_name'}, inplace=True)\n",
    "    \n",
    "        plt.clf()\n",
    "        g = sns.barplot(data[var], data.my_name, orient='h', palette=\"spring\" )\n",
    "        for index, row in data[var].iteritems():\n",
    "            g.text(row, index, round(row,3), color='black', ha=\"right\") \n",
    "        plt.title(var.upper())\n",
    "        \n",
    "    def make_models_list(self, chosen):\n",
    "        '''return a list of tuples for Voting. Chosen must be a list'''\n",
    "        data = pd.DataFrame(Super_Analytica.evaluations_all).T\n",
    "        s = data.loc[chosen, ['best_estimator']] # select the column of interest\n",
    "        s = s['best_estimator'].map(lambda x: x.steps[-1][1]) # within best_estimator object, retrieve the model instance\n",
    "        return list(s.iteritems())\n",
    "    \n",
    "    \n",
    "    def extract_model_instance(self, my_name):\n",
    "        '''my_name must be a single value string. \n",
    "        Returns the model instance out of the pipeline. ( pipelilne stored in evaluaions all) \n",
    "        Intended to export the fitted model and it`s best perams for reuse (bagging).\n",
    "        '''\n",
    "#         my_name_no_under = my_name.split('_')[0]\n",
    "#         return Super_Analytica.evaluations_all[my_name]['best_estimator'].named_steps[my_name_no_under]\n",
    "        return master.evaluations_all[my_name]['best_estimator']\n",
    "\n",
    "    def pkl_W_evaluations_all(self, filename):    # save evaluation all to disk\n",
    "        '''filename is a string like KNN_1'''\n",
    "        pickle_w = open('../pkl/'+ filename + '.pkl', 'wb')\n",
    "        obj = Super_Analytica.evaluations_all\n",
    "        pickle.dump(obj, pickle_w, pickle.HIGHEST_PROTOCOL)\n",
    "        pickle_w.close()\n",
    "        print('Done. Pickled to disk')\n",
    "        \n",
    "    def pkl_R(self, filename):     # import a pickle into the 'cloud'\n",
    "        ''' filename must be a string like KNN_1\n",
    "        import a pickle into the 'cloud'\n",
    "        '''\n",
    "#         pickle_r = open('../pkl/'+ filename + '.pkl', 'rb')\n",
    "#         back_from_dead = pickle.load(pickle_r)\n",
    "        back_from_dead = joblib.load('../pkl/clf_'+ filename + '.pkl') \n",
    "        #check duplicates and insert into evaluations_all\n",
    "        keys_to_import = list(back_from_dead.keys())\n",
    "        keys_members = list(Super_Analytica.evaluations_all.keys())\n",
    "        Super_Analytica.evaluations_all.update(back_from_dead)\n",
    "        print('Done. ' + filename + ' evaluation pickled from pisk')\n",
    "        \n",
    "        if any(i in keys_members for i in keys_to_import):\n",
    "            print (' One or more evaluations were overwriten')\n",
    "   \n",
    "        \n",
    "    def get_members_evaluations_all(self):\n",
    "        return Super_Analytica.evaluations_all.keys() # pega somente os modelos que acionaram evaluate_HP\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            ##############     PLOTTING SECTION     ###############\n",
    "    \n",
    "    def plot_learning_curve(self, load_saved=True, chosen=None):  #loop over\n",
    "        '''\n",
    "        If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list of members of evaluations_all\n",
    "        if chosen == None, it will process all the models in evaluations all\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members)) # all chosen in evaluations_all\n",
    "            list_members = chosen # check if chosen are in evaluations all\n",
    "            list_members.sort()     \n",
    "        \n",
    "        for name in list_members :\n",
    "            path = '../pkl/l_curve_'+ name + '.png'\n",
    "            if load_saved == True:\n",
    "#                 print(type(name))\n",
    "                print('opening the image',  name)\n",
    "        \n",
    "                display(Image(path))\n",
    "            else:\n",
    "                pipe = Super_Analytica.evaluations_all[name]['pipe_best']\n",
    "                self.helper_plot_learning_curve(pipe, name, path)\n",
    "            \n",
    "\n",
    "    def helper_plot_learning_curve(self, pipe, name, path):    \n",
    "        sizes = np.linspace(0.2, 1.0, 5)\n",
    "        viz = LearningCurve(pipe, train_sizes=sizes, scoring='roc_auc')\n",
    "        viz.fit(X_train, y_train)\n",
    "        viz.poof(outpath=path)\n",
    "        print('saving l_curve_ image for ',  name)\n",
    "        plt.clf()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def plot_validation_curve(self, chosen=None):\n",
    "        '''\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "        \n",
    "        for name in list_members:\n",
    "            self.helper_plot_validation_curve(name)\n",
    "        \n",
    "    \n",
    "    def helper_plot_validation_curve(self, my_name):\n",
    "        \n",
    "        data4 = Super_Analytica.helper_translate(self, my_name)\n",
    "        for param2, vals2 in data4.iteritems(): #iterar sobre o data4 que é o geral que contem todas as colunas\n",
    "#             print('param2: ',  param2)\n",
    "            if param2 == 'losses':\n",
    "                continue\n",
    "            \n",
    "            ### HYPEROPT VALIDATION CURVE - LOSS FUNCTION\n",
    "            data_temp = None # used exclusively on HERE \n",
    "            data_temp = data4[[ param2, 'losses']].dropna().copy()\n",
    "#             print('param2__: ',  param2)\n",
    "            x = data_temp[param2]\n",
    "            y = data_temp.losses\n",
    "            plt.scatter(x,y, alpha=0.2, c=y, cmap='RdBu')\n",
    "            plt.xlabel(param2)\n",
    "            plt.ylabel('Score')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "   \n",
    "    def plot_models_corr(self, chosen=None):\n",
    "        if chosen:\n",
    "            corr = pd.DataFrame({k:v['y_pred'] for k,v in self.evaluations_all.items() if k in chosen}).corr()\n",
    "        else:\n",
    "            corr = pd.DataFrame({k:v['y_pred'] for k,v in self.evaluations_all.items()}).corr()\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(14,10)); \n",
    "        sns.heatmap(corr, annot=True, cmap=\"YlGnBu\", ax=ax)    \n",
    "\n",
    "        \n",
    "        \n",
    "    def plot_Classification_Report(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "\n",
    "        for name in list_members:\n",
    "            path = '../pkl/c_reprt_'+ name + '.png'\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "            \n",
    "                display(Image(path))\n",
    "            else:\n",
    "\n",
    "                pipe = self.evaluations_all[name]['pipe_best']\n",
    "                visualizer = ClassificationReport(pipe, support=True)\n",
    "\n",
    "                visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "                visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "                visualizer.poof(outpath=path)\n",
    "                plt.clf()\n",
    "                print('saving c_reprt_ image for',  name)       \n",
    "            \n",
    "\n",
    "    def plot_Confusion_Matrix(self, load_saved=True, chosen=None):\n",
    "        '''If load_saved == False it will overwrite any file named equal\n",
    "        chosen must be a list\n",
    "        '''\n",
    "    \n",
    "        list_members = self.get_members_evaluations_all()\n",
    "        if chosen:\n",
    "            assert set(chosen).issubset(set(list_members))\n",
    "            list_members = chosen \n",
    "\n",
    "        for name in list_members:\n",
    "            path = '../pkl/c_matrx_'+ name + '.png'\n",
    "            if load_saved == True:\n",
    "                print('opening the image',  name)\n",
    "                display(Image(path))\n",
    "\n",
    "            else:\n",
    "                pipe = self.evaluations_all[name]['pipe_best']\n",
    "                visualizer = ConfusionMatrix(pipe, percent=True)\n",
    "                visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "                visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "                visualizer.poof(outpath=path) \n",
    "                plt.clf()\n",
    "                print('saving c_matrx_ image for',  name)       \n",
    "            \n",
    "             \n",
    "    def explain_weights(self, name):\n",
    "        '''ELI 5'''\n",
    "        est = Super_Analytica.evaluations_all[name]['instance']\n",
    "        vec = map_tfidf.features[0][1][1]\n",
    "        return eli5.show_weights(est, vec=vec)\n",
    "\n",
    "    \n",
    "    \n",
    "    def explain_prediction(self, name):\n",
    "        '''ELI 5'''\n",
    "        est = Super_Analytica.evaluations_all[name]['instance']\n",
    "        vec = map_tfidf.features[0][1][1]\n",
    "        return eli5.show_prediction(est, X_train.iloc[1,:][0] , vec=vec)\n",
    "\n",
    "                \n",
    "##############     EVALUATION SECTION     ###############\n",
    "    \n",
    "    def evaluate_CV(self, pipe, chosen, sufix='', pkl_W=False, vectorize_first=False):\n",
    "        ''' iterates over the dict of models clf_all and not over Evaluations_all\n",
    "            chosen must be a list of elements from clf_all. \n",
    "            sufix must be a string. KNN_1234 --> KNN_sufix\n",
    "            the input pipe must not have the last step. It will be placed dynamically by pipe.append.\n",
    "            and applied to all chosen.\n",
    "        '''\n",
    "        assert set(chosen).issubset(set(clf_all.keys())) # all elements in chosen are in clf_all\n",
    "      \n",
    "        for name in chosen:\n",
    "            start = timer()\n",
    "\n",
    "            pipe_full = clone(pipe)\n",
    "            pipe_full.steps.append([name, clf_all[name]])\n",
    "        \n",
    "            if vectorize_first==True:\n",
    "                X_train_ = pipe.fit_transform(X_train, y_train)   \n",
    "                obj_1 = clf_all[name]\n",
    "\n",
    "            else:\n",
    "                X_train_ = X_train\n",
    "                X_test_ = X_test\n",
    "                obj_1 = pipe_full\n",
    "            \n",
    "            y_pred = None\n",
    "            acc = cross_val_score(obj_1, X_train_, y_train, scoring='roc_auc', cv=3).mean() \n",
    "            end = timer()\n",
    "            minutes_elapsed = (end - start)//60\n",
    "            name = name + sufix \n",
    "            Super_Analytica.evaluations_all[name] = {'score': acc, 'best_estimator': None, \n",
    "                                                             'param_grid': None, 'y_pred': y_pred, \n",
    "                                                             'pipe_best': pipe_full, 'instance':None,\n",
    "                                                             'best_params' : None, 'T': None, 'minutes': minutes_elapsed }             \n",
    "            if  pkl_W:\n",
    "                D = {}\n",
    "                D[name]= Super_Analytica.evaluations_all[name]\n",
    "                joblib.dump(D, '../pkl/clf_'+ name + '.pkl')        \n",
    "                print(name, '  Cross Validation and Pickled Done in ', minutes_elapsed, ' minutes', ' /-->acc :', acc , '\\n') \n",
    "            else:\n",
    "                print(name, '  Cross Validation Done in ', minutes_elapsed, ' minutes', ' /-->acc :', acc , '\\n') \n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "    def helper_translate(self, my_name):   #plot_validation_curve\n",
    "        # translates FROM STEPS(len) TO the chosen range of VALUES\n",
    "        data4 = Super_Analytica.helper_translate_2(self, my_name)\n",
    "        P = Super_Analytica.evaluations_all[my_name]['param_grid']\n",
    "        hyperopt_list = [hyperopt.pyll_utils.str(v) for k,v in P.items()] # abrindo o hyperopt.utils numa lista of long strings. one string for each param\n",
    "    #     print(hyperopt_list)\n",
    "\n",
    "        def helper_split_string(s): # breaks up the long string and remove ''. returns a list of strings\n",
    "            ls = [i for i in s.split(' ')] \n",
    "            for i in range(len(ls)):\n",
    "                if '' in ls:\n",
    "                    ls.remove('')\n",
    "            return ls\n",
    "\n",
    "        hyperopt_list2 = list(map(helper_split_string, hyperopt_list))   # lista hyperopt.utils ready to be analysed for patterns\n",
    "#         print(hyperopt_list2)\n",
    "\n",
    "        pat = r'\\w+\\{(\\w+)\\}\\W+[n3]'\n",
    "#         pat2 = r'\\w+\\{(\\d+)\\}'\n",
    "        pat2 = r'\\{([0-9]*[.]?[0-9]+)\\}'\n",
    "\n",
    "\n",
    "        D = {}     # dict to store the whole range of real values\n",
    "        for L in hyperopt_list2:  # procurar pelo param que comeca com X e retrieve values\n",
    "            if len(L) == 1:\n",
    "                break\n",
    "            else:\n",
    "                w = L[3]  # captura o 4 termo\n",
    "                if re.search(pat, w): # checa se dentro dos {} tem uma palavra\n",
    "            #         print('yes')\n",
    "                    m = re.search(pat, w).groups()[0] # var com o termo capturado, que é um key de um dos dicts  de param_1\n",
    "                    if m.startswith('zz_'):  # Checa se o termo capturado inicia com 'zz_'. CAso sim, processa adiante.\n",
    "#                         print(m)\n",
    "#                         print(L[6:])\n",
    "                        D[m] = [re.search(pat2, i).groups()[0] for i in L[6:]] # varre todos os mebros apos o 6 membro capturando o integer via o pat2. coleta todo o range de valores para cada param. Cada key é um param com prefixo 'X'.\n",
    "            #             print(D[m])\n",
    "                        data4[m] = data4[m].map(lambda x: D[m][int(x)]).astype('float64') # map each value x of the given column data4[m] with the index x in the respect dict D[m]. x represent the step     update the column in data4. Columns have 'X' prefix\n",
    "        data4.columns = data4.columns.map(lambda x: x.strip('X') if x.startswith('X') else x) # renomeia as colunas para nao ter X\n",
    "        data4 = data4.reindex(sorted(data4.columns), axis=1) # order the columns afabetically\n",
    "#         print('end translate')\n",
    "        ### END DATA PREP\n",
    "        return data4\n",
    "    \n",
    "    \n",
    "    def helper_translate_2(self, my_name):   # plot_validation_curve\n",
    "        '''ingest the Trials dict, clean it and return a Data Frame with the sampled values to be updated ahead.'''\n",
    "\n",
    "        data3 = Super_Analytica.evaluations_all[my_name]['T']\n",
    "    #     data3\n",
    "\n",
    "        d = [i['misc']['vals'] for i in  data3.trials]\n",
    "\n",
    "        L = []\n",
    "        for i in  d:\n",
    "            d_transformed = {k:v[0] if v !=[] else v  for k,v in i.items()} # retirando os valores de dentro das lista.\n",
    "            L.append(d_transformed)\n",
    "\n",
    "        param_list = [list(i['misc']['vals'].keys())  for i in data3.trials[:1]][0]\n",
    "        param_list_noChoices =  [i for i in param_list if i != 'choices' ]\n",
    "    #     param_list\n",
    "    #     param_list_noChoices\n",
    "\n",
    "        data4 = pd.DataFrame(L)\n",
    "        data4['losses'] = [1-i for i in data3.losses()] # invertendo de losses --> score\n",
    "        data4 = data4.applymap(lambda x: np.nan if x == [] else x) # transforma os [] em NaN - para qnd houver choice boladao\n",
    "        return data4 # este dataset contem as colunas'X' que deverao ser traduzidas posteriormente\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T03:54:32.442429Z",
     "start_time": "2018-05-19T03:54:32.411577Z"
    },
    "code_folding": [
     13,
     61
    ]
   },
   "outputs": [],
   "source": [
    "class Analytica(object):\n",
    "    \n",
    "    def __init__(self, instance, param_grid, my_name):\n",
    "        self.my_name = my_name\n",
    "        self.instance = instance    \n",
    "        self.param_grid = param_grid\n",
    "        Super_Analytica.insert_member(self)\n",
    "        \n",
    " \n",
    " \n",
    "    def evaluate_HP_cv(self, pipe, pkl_W=False): \n",
    "        '''       \n",
    "            Essa funcao realiza parameter tunning via cross validation e aplica best_model em todo dataset de treino.\n",
    "            O parametro 'pipe' nao deve conter um classificador como ultimo elemento.\n",
    "        '''\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "\n",
    "        pipe_full = clone(pipe)\n",
    "        pipe_full.steps.append([instance, clf_all[instance]])\n",
    "\n",
    "       \n",
    "        def objective(params):\n",
    "            pipe_full.set_params(**params) \n",
    "            score = cross_val_score(pipe_full, X_train, y_train, scoring='roc_auc', cv=kfold)\n",
    "            print('cv score:', score.mean())\n",
    "            return 1 - score.mean()\n",
    "        \n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=2, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) # is the translation of best. Real Values, not steps.\n",
    "        pipe_full.set_params(**best_params)\n",
    "        clf_ = pipe_full.fit(X_train, y_train) \n",
    "\n",
    "        y_pred = clf_.predict_proba(X_test)[:, 1]\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('test score :', acc)\n",
    "        \n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        \n",
    "        # record all scores to evaluations_all\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': clf_, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "                                 'pipe_best': pipe_full, 'instance':instance,\\\n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "                \n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP_cv and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP_cv done in ', minutes_elapsed, ' minutes') \n",
    "      \n",
    "    \n",
    "\n",
    "            \n",
    "######################        PARFIT       ######################\n",
    "            \n",
    "    def evaluate_Parfit(self, pipe, pkl_W=False): \n",
    "        '''\n",
    "        Essa funcao realiza parameter tunning via validation set e aplica best_model em todo dataset treino.\n",
    "        O pipeline 'pipe' nao deve conter um classificador como ultimo elemento.\n",
    "        X_train, X_val, X_test serao vetorizados de acordo com o parametro 'pipe'.\n",
    "        O praram_grid deve conter ParameterGrid({})\n",
    "            \n",
    "        '''\n",
    "        start= timer()\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid        \n",
    "\n",
    "        X_train_ = pipe.fit_transform(X_train, y_train) \n",
    "        X_val_ = pipe.transform(X_val)\n",
    "        X_test_ = pipe.transform(X_test)\n",
    "        \n",
    "        best_model, best_score, all_models, all_scores = pf.bestFit(clf_all[instance], \\\n",
    "                                                                     \n",
    "            param_grid, X_train_, y_train, X_val_, y_val,\\\n",
    "            metric=roc_auc_score, scoreLabel='AUC')\n",
    "#         print(best_model)\n",
    "        \n",
    "        mdl = best_model.fit(X_train_, y_train) \n",
    "        y_pred = mdl.predict_proba(X_test_)[:, 1]\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('acc :', acc)\n",
    "\n",
    "        \n",
    "        pipe_best = clone(pipe)\n",
    "        pipe_best.steps.append([self.my_name, best_model])\n",
    "\n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': best_model , \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \\\n",
    "                                 'pipe_best': pipe_best, 'instance':instance,\\\n",
    "                                 'best_params' : best_model, 'T': None, 'minutes': minutes_elapsed} \n",
    "          \n",
    "        print('start pickling')\n",
    "        if pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl')     \n",
    "            print(self.my_name, ' Parfit and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  Parfit done in ', minutes_elapsed, ' minutes') \n",
    "        \n",
    "    \n",
    "    \n",
    "######################        LIGHT GBM         ########################\n",
    "         \n",
    "    \n",
    "    def evaluate_HP_lGBM(self, pipe, pkl_W=False): \n",
    "        '''\n",
    "            This function uses HyperOpt to tune LightGHM params on train and validation set. Than it refits the best model on entire trainset and score on testset.\n",
    "            O pipeline 'pipe' nao deve conter um classificador como ultimo elemento.\n",
    "        '''\n",
    "        start= timer()\n",
    "        result_dict = {}\n",
    "\n",
    "        instance = self.instance\n",
    "        param_grid = self.param_grid\n",
    "\n",
    "\n",
    "        def objective(params):\n",
    "            # Make sure parameters that need to be integers are integers\n",
    "            for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "                params[parameter_name] = int(params[parameter_name])\n",
    "\n",
    "            model = clf_all[instance].set_params(**params)\n",
    "            model.fit(train_feature_matrix, y_train, eval_set=(val_feature_matrix, y_val),\\\n",
    "                      eval_names=['val_feature_matrix', 'y_val'], eval_metric='auc', early_stopping_rounds=10, verbose=False)             \n",
    "            best_score = model.best_score_['val_feature_matrix']['auc']\n",
    "#             print('val score: ', best_score)\n",
    "            return 1 - best_score\n",
    "\n",
    "\n",
    "        train_feature_matrix = pipe.fit_transform(X_train, y_train) # pipe descalco sem o ultimo\n",
    "        data_train = lgb.Dataset(train_feature_matrix, label=list(y_train.values.ravel()))\n",
    "\n",
    "        test_feature_matrix = pipe.transform(X_test) # pipe sem o ultimo\n",
    "        data_test = lgb.Dataset(test_feature_matrix, label=list(y_test.values.ravel()))\n",
    "        \n",
    "        val_feature_matrix = pipe.transform(X_val) \n",
    "\n",
    "        T = Trials()\n",
    "        space = param_grid\n",
    "        best = fmin(objective, space, algo=tpe.suggest, max_evals=30, trials=T)  #  <---------------------------HERE<-----\n",
    "        best_params = space_eval(space, best) \n",
    "\n",
    "        for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "                best_params[parameter_name] = int(best_params[parameter_name])\n",
    "\n",
    "        best_model = clf_all[instance].set_params(**best_params)\n",
    "        best_model.fit(train_feature_matrix, y_train.values.ravel())\n",
    "        y_pred = best_model.predict_proba(test_feature_matrix)[:,1]\n",
    "#         y_pred = best_model.predict(test_feature_matrix)\n",
    "#         print(y_pred)\n",
    "        acc = roc_auc_score(y_test, y_pred) \n",
    "        print('test score :', acc)\n",
    "        \n",
    "        pipe_best = clone(pipe)\n",
    "        pipe_best.steps.append([self.my_name, best_model])\n",
    "#         print('pipe_best :', pipe_best.steps)\n",
    "        \n",
    "        end = timer()\n",
    "        minutes_elapsed = (end - start)//60\n",
    "        Super_Analytica.evaluations_all[self.my_name] = {'score': acc, 'best_estimator': best_model, \\\n",
    "                                 'param_grid': param_grid, 'y_pred': y_pred, \n",
    "                                 'pipe_best': pipe_best, 'instance':instance, # é somente uma string, nao é a instance em si\\ \n",
    "                                 'best_params' : best_params, 'T': T, 'minutes': minutes_elapsed} \n",
    "\n",
    "\n",
    "        if  pkl_W:\n",
    "            D = {}\n",
    "            D[self.my_name]= Super_Analytica.evaluations_all[self.my_name]\n",
    "            joblib.dump(D, '../pkl/clf_'+ self.my_name + '.pkl') \n",
    "            print(self.my_name, ' HP_lightGBM and pickled to disk Done in',  minutes_elapsed, ' minutes' ) \n",
    "        else:\n",
    "            print(self.my_name, '  HP_lightGBM done in ', minutes_elapsed, ' minutes') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_models(s):\n",
    "    '''utility helper function to look in md_all for the models that starts with s return it in a list os strings '''\n",
    "    all_names = [name for L in md_all for name in L] # from list of lists to a single list\n",
    "    return [name for name in all_names if name.startswith(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "333px",
    "left": "1376px",
    "top": "505px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
